import copy
import enum
from sklearn.decomposition import PCA
import torch
import numpy as np
import math
from scipy import stats
from functools import reduce
import time
import sklearn.metrics.pairwise as smp
import hdbscan
from scipy.spatial.distance import cdist
from scipy.stats import entropy
from sklearn.cluster import KMeans
from utils import *
from sklearn.preprocessing import StandardScaler
import torch.nn as nn
import torch.optim as optim
# æ·»åŠ å¯è§†åŒ–ç›¸å…³å¯¼å…¥
import matplotlib
matplotlib.use('Agg')  # è®¾ç½®åç«¯ï¼Œæ”¯æŒæ— GUIç¯å¢ƒ
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
import os
from matplotlib.backends.backend_pdf import PdfPages

# è®¾ç½®matplotlibå’Œseabornæ ·å¼
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['font.size'] = 10
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.bbox'] = 'tight'

def get_pca(data, threshold = 0.99):
    normalized_data = StandardScaler().fit_transform(data)
    pca = PCA()
    reduced_data = pca.fit_transform(normalized_data)
    # Determine explained variance using explained_variance_ration_ attribute
    exp_var = pca.explained_variance_ratio_
    cum_sum_eigenvalues = np.cumsum(exp_var)
    select_pcas = np.where(cum_sum_eigenvalues <=threshold)[0]
    # print('Number of components with variance <= {:0.0f}%: {}'.format(threshold*100, len(select_pcas)))
    reduced_data = reduced_data[:, select_pcas]
    return reduced_data

eps = np.finfo(float).eps

class LFD():
    def __init__(self, num_classes, enable_visualization=True, save_path="./figures/", 
                 visualization_frequency=1, max_visualizations=0, save_final_only=False,
                 save_as_pdf=True, keep_individual_files=False, attack_ratio=None):
        """
        Args:
            save_as_pdf: æ˜¯å¦ä¿å­˜ä¸ºPDFæ ¼å¼ï¼ˆé»˜è®¤Trueï¼‰
            keep_individual_files: æ˜¯å¦ä¿ç•™å•ç‹¬çš„PNGæ–‡ä»¶ï¼ˆé»˜è®¤Falseï¼‰
            attack_ratio: æ”»å‡»è€…æ¯”ç‡ï¼Œç”¨äºPDFæ–‡ä»¶å
        """
        self.num_classes = num_classes
        self.memory = np.zeros(num_classes)
        
        # å¯è§†åŒ–ç›¸å…³å‚æ•°
        self.enable_visualization = enable_visualization
        self.save_path = save_path
        self.visualization_frequency = visualization_frequency
        self.max_visualizations = max_visualizations
        self.save_final_only = save_final_only
        self.save_as_pdf = save_as_pdf
        self.keep_individual_files = keep_individual_files
        self.attack_ratio = attack_ratio
        
        # è½®æ•°è®¡æ•°å™¨
        self.round_counter = 0
        self.total_rounds = None
        self.visualization_count = 0
        
        # PDFæ–‡ä»¶ç®¡ç†
        self.pdf_pages = None
        self.pdf_filename = None
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        if self.enable_visualization:
            os.makedirs(self.save_path, exist_ok=True)
            
            # åˆ›å»ºPDFæ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨PDFä¿å­˜ï¼‰
            if self.save_as_pdf:
                self._initialize_pdf()
    
    def _initialize_pdf(self):
        """åˆå§‹åŒ–PDFæ–‡ä»¶"""
        try:
            from matplotlib.backends.backend_pdf import PdfPages
            attack_str = f"_atr{self.attack_ratio:.1f}" if self.attack_ratio is not None else ""
            self.pdf_filename = f'{self.save_path}/lfighter_complete_report{attack_str}.pdf'
            self.pdf_pages = PdfPages(self.pdf_filename)
            print(f"[LFighter] ğŸ”— PDFæŠ¥å‘Šåˆå§‹åŒ–æˆåŠŸ")
            print(f"[LFighter] ğŸ“„ å®æ—¶æŸ¥çœ‹è·¯å¾„: {self.pdf_filename}")
            print(f"[LFighter] ğŸ’¡ æç¤º: æ¯ä¸ªepochåPDFä¼šè‡ªåŠ¨æ›´æ–°ï¼Œå¯éšæ—¶æŸ¥çœ‹")
        except Exception as e:
            print(f"[LFighter] PDFåˆå§‹åŒ–å¤±è´¥: {e}")
            self.pdf_pages = None
    
    def finalize_pdf(self):
        """å…³é—­PDFæ–‡ä»¶"""
        if self.pdf_pages is not None:
            try:
                self.pdf_pages.close()
                print(f"[LFighter] PDFæŠ¥å‘Šå·²ä¿å­˜: {self.pdf_filename}")
            except Exception as e:
                print(f"[LFighter] PDFå…³é—­å¤±è´¥: {e}")
            finally:
                self.pdf_pages = None
    
    def set_total_rounds(self, total_rounds):
        """è®¾ç½®æ€»è®­ç»ƒè½®æ•°ï¼Œç”¨äºsave_final_onlyæ¨¡å¼"""
        self.total_rounds = total_rounds
        if self.enable_visualization:
            print(f"[LFighter] Set total rounds to {total_rounds} for visualization control")
    
    def should_visualize_this_round(self):
        """åˆ¤æ–­å½“å‰è½®æ¬¡æ˜¯å¦åº”è¯¥ä¿å­˜å¯è§†åŒ–"""
        if not self.enable_visualization:
            return False
            
        # å¦‚æœè®¾ç½®ä¸ºåªä¿å­˜æœ€åä¸€è½®
        if self.save_final_only:
            return self.total_rounds is not None and self.round_counter == self.total_rounds
        
        # åŠ¨æ€è°ƒæ•´å¯è§†åŒ–é¢‘ç‡ï¼šå‰20è½®æ¯è½®å¯è§†åŒ–ï¼Œåç»­æ¯10è½®ä¸€æ¬¡
        if self.round_counter <= 20:
            current_frequency = 1
        else:
            current_frequency = 10
        
        # æŒ‰åŠ¨æ€é¢‘ç‡ä¿å­˜
        return self.round_counter % current_frequency == 0
    
    def cleanup_old_visualizations(self):
        """æ¸…ç†æ—§çš„å¯è§†åŒ–æ–‡ä»¶"""
        if not self.enable_visualization:
            return
        
        if self.max_visualizations > 0 and self.visualization_count > self.max_visualizations:
            # æ¸…ç†PNGæ–‡ä»¶
            png_files = [f for f in os.listdir(self.save_path) if f.startswith('lfighter_') and f.endswith('.png')]
            png_files.sort(key=lambda x: os.path.getctime(os.path.join(self.save_path, x)))
            
            while len(png_files) > self.max_visualizations:
                old_file = os.path.join(self.save_path, png_files.pop(0))
                if os.path.exists(old_file):
                    os.remove(old_file)
            
            # æ¸…ç†æ–‡æœ¬æ–‡ä»¶
            txt_files = [f for f in os.listdir(self.save_path) if f.startswith('lfighter_') and f.endswith('.txt')]
            txt_files.sort(key=lambda x: os.path.getctime(os.path.join(self.save_path, x)))
            
            while len(txt_files) > self.max_visualizations:
                old_file = os.path.join(self.save_path, txt_files.pop(0))
                if os.path.exists(old_file):
                    os.remove(old_file)
    
    def create_pdf_report(self, round_num, features, labels, ptypes, scores, cs0, cs1, good_cl, metrics):
        """å‘å·²æœ‰PDFæ–‡ä»¶æ·»åŠ å½“å‰è½®æ¬¡çš„å¯è§†åŒ–é¡µé¢"""
        if not self.should_visualize_this_round() or not self.save_as_pdf or self.pdf_pages is None:
            return
        
        try:
            # ç¬¬ä¸€é¡µï¼šç‰¹å¾ç©ºé—´å¯è§†åŒ–
            fig, axes = plt.subplots(1, 2, figsize=(16, 8))
            
            # t-SNEé™ç»´
            if features.shape[1] > 2:
                try:
                    perplexity = min(30, len(features)-1)
                    if perplexity < 1:
                        perplexity = 1
                    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
                    features_2d = tsne.fit_transform(features)
                except:
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=2, random_state=42)
                    features_2d = pca.fit_transform(features)
            else:
                features_2d = features
            
            # æŒ‰å®¢æˆ·ç«¯ç±»å‹ç€è‰²
            colors = []
            malicious_count = 0
            for ptype in ptypes:
                ptype_str = str(ptype).lower()
                if ('malicious' in ptype_str or 'attack' in ptype_str or 
                    'bad' in ptype_str or 'adversarial' in ptype_str):
                    colors.append('red')
                    malicious_count += 1
                else:
                    colors.append('blue')
            
            axes[0].scatter(features_2d[:, 0], features_2d[:, 1], c=colors, alpha=0.7, s=100)
            axes[0].set_title('LFighter: Feature Space (by Client Type)', fontsize=14)
            axes[0].set_xlabel('Dimension 1')
            axes[0].set_ylabel('Dimension 2')
            
            red_patch = plt.matplotlib.patches.Patch(color='red', label='Malicious')
            blue_patch = plt.matplotlib.patches.Patch(color='blue', label='Benign')
            axes[0].legend(handles=[red_patch, blue_patch])
            
            # æŒ‰èšç±»ç»“æœç€è‰²
            cluster_colors = ['orange', 'green']
            for i in range(len(features_2d)):
                axes[1].scatter(features_2d[i, 0], features_2d[i, 1], 
                               c=cluster_colors[labels[i]], alpha=0.7, s=100)
            axes[1].set_title('LFighter: Feature Space (by Cluster)', fontsize=14)
            axes[1].set_xlabel('Dimension 1')
            axes[1].set_ylabel('Dimension 2')
            
            orange_patch = plt.matplotlib.patches.Patch(color='orange', label='Cluster 0')
            green_patch = plt.matplotlib.patches.Patch(color='green', label='Cluster 1')
            axes[1].legend(handles=[orange_patch, green_patch])
            
            plt.suptitle(f'LFighter Algorithm - Round {round_num} - Feature Space Analysis', fontsize=16)
            plt.tight_layout()
            self.pdf_pages.savefig(fig, bbox_inches='tight')
            plt.close()
            
            # ç¬¬äºŒé¡µï¼šèšç±»è´¨é‡å’Œå®¢æˆ·ç«¯å¾—åˆ†
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            
            # èšç±»è´¨é‡
            clusters = ['Cluster 0', 'Cluster 1']
            dissimilarities = [cs0, cs1]
            cluster_quality_colors = ['green' if i == good_cl else 'red' for i in range(2)]
            
            bars = axes[0, 0].bar(clusters, dissimilarities, color=cluster_quality_colors, alpha=0.7)
            axes[0, 0].set_ylabel('Dissimilarity Score')
            axes[0, 0].set_title('Cluster Quality Comparison')
            axes[0, 0].grid(True, alpha=0.3)
            
            for i, (bar, score) in enumerate(zip(bars, dissimilarities)):
                axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                              f'{score:.4f}', ha='center', va='bottom')
            
            axes[0, 0].text(good_cl, dissimilarities[good_cl] + 0.05, 'âœ“ Good Cluster', 
                           ha='center', va='bottom', fontsize=12, fontweight='bold')
            
            # å®¢æˆ·ç«¯å¾—åˆ†æ¡å½¢å›¾
            client_indices = range(len(scores))
            bars = axes[0, 1].bar(client_indices, scores, color=colors, alpha=0.7)
            axes[0, 1].set_title('Client Scores')
            axes[0, 1].set_xlabel('Client Index')
            axes[0, 1].set_ylabel('Score')
            axes[0, 1].set_ylim(0, 1.1)
            axes[0, 1].grid(True, alpha=0.3)
            
            # å¾—åˆ†åˆ†å¸ƒç›´æ–¹å›¾
            axes[1, 0].hist(scores, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
            axes[1, 0].axvline(np.mean(scores), color='red', linestyle='--', 
                              label=f'Mean: {np.mean(scores):.4f}')
            axes[1, 0].axvline(np.median(scores), color='orange', linestyle='--', 
                              label=f'Median: {np.median(scores):.4f}')
            axes[1, 0].set_xlabel('Score')
            axes[1, 0].set_ylabel('Frequency')
            axes[1, 0].set_title('Score Distribution')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
            
            # æ€§èƒ½æŒ‡æ ‡æ–‡æœ¬
            axes[1, 1].axis('off')
            attack_ratio_str = f"Attack Ratio: {self.attack_ratio:.1f}" if self.attack_ratio is not None else "Attack Ratio: N/A"
            metrics_text = f"""
Performance Metrics - Round {round_num}

{attack_ratio_str}
Total Clients: {metrics.get('total_clients', 'N/A')}
Good Clients Selected: {metrics.get('good_clients', 'N/A')}
Selection Accuracy: {metrics.get('accuracy', 'N/A'):.2%}

Cluster Analysis:
â€¢ Cluster 0 Dissimilarity: {metrics.get('cs0', 'N/A'):.4f}
â€¢ Cluster 1 Dissimilarity: {metrics.get('cs1', 'N/A'):.4f}
â€¢ Good Cluster: {metrics.get('good_cluster', 'N/A')}

Feature Processing:
â€¢ Reduction Method: {metrics.get('reduction_method', 'N/A')}
â€¢ Detected Anomalous Classes: {metrics.get('anomalous_classes', 'N/A')}

Algorithm: Original LFighter (K-means clustering)
Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}
"""
            axes[1, 1].text(0.05, 0.95, metrics_text, transform=axes[1, 1].transAxes, 
                           fontsize=10, verticalalignment='top', fontfamily='monospace')
            
            plt.suptitle(f'LFighter Algorithm - Round {round_num} - Performance Analysis', fontsize=16)
            plt.tight_layout()
            self.pdf_pages.savefig(fig, bbox_inches='tight')
            plt.close()
            
            # å¼ºåˆ¶åˆ·æ–°PDFæ–‡ä»¶ï¼Œç¡®ä¿å®æ—¶å¯è§
            try:
                # matplotlib PdfPages çš„æ­£ç¡®flushæ–¹æ³•
                if hasattr(self.pdf_pages, '_file') and hasattr(self.pdf_pages._file, '_file'):
                    self.pdf_pages._file._file.flush()
                    import os
                    os.fsync(self.pdf_pages._file._file.fileno())
                elif hasattr(self.pdf_pages, 'flush'):
                    self.pdf_pages.flush()
            except:
                pass  # å¿½ç•¥flushé”™è¯¯ï¼Œä¸å½±å“æ­£å¸¸åŠŸèƒ½
            
            print(f"[LFighter] Round {round_num} æ·»åŠ åˆ°PDFæŠ¥å‘Š - å®æ—¶å¯æŸ¥çœ‹: {self.pdf_filename}")
            
        except Exception as e:
            print(f"[LFighter] PDFé¡µé¢æ·»åŠ å¤±è´¥ Round {round_num}: {e}")
    
    def visualize_feature_space(self, features, labels, ptypes, round_num):
        """å¯è§†åŒ–åŸå§‹ç‰¹å¾ç©ºé—´"""
        if not self.should_visualize_this_round():
            return
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å®¢æˆ·ç«¯ç±»å‹
        if self.enable_visualization and round_num == 1:  # åªåœ¨ç¬¬ä¸€è½®æ‰“å°
            print(f"[LFighter Debug] Client types: {ptypes[:10]}...")  # æ‰“å°å‰10ä¸ªå®¢æˆ·ç«¯ç±»å‹
        
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # t-SNEé™ç»´å¯è§†åŒ–
        if features.shape[1] > 2:
            try:
                perplexity = min(30, len(features)-1)
                if perplexity < 1:
                    perplexity = 1
                tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
                features_2d = tsne.fit_transform(features)
            except:
                # å¦‚æœt-SNEå¤±è´¥ï¼Œä½¿ç”¨PCA
                from sklearn.decomposition import PCA
                pca = PCA(n_components=2, random_state=42)
                features_2d = pca.fit_transform(features)
        else:
            features_2d = features
        
        # 1. æŒ‰å®¢æˆ·ç«¯ç±»å‹ç€è‰² - å¢å¼ºæ£€æµ‹é€»è¾‘
        colors = []
        malicious_count = 0
        for ptype in ptypes:
            ptype_str = str(ptype).lower()
            if ('malicious' in ptype_str or 'attack' in ptype_str or 
                'bad' in ptype_str or 'adversarial' in ptype_str):
                colors.append('red')
                malicious_count += 1
            else:
                colors.append('blue')
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ£€æµ‹ç»“æœ
        if self.enable_visualization and round_num == 1:
            print(f"[LFighter Debug] Detected {malicious_count}/{len(ptypes)} malicious clients")
        
        axes[0].scatter(features_2d[:, 0], features_2d[:, 1], c=colors, alpha=0.7, s=100)
        axes[0].set_title('LFighter: Feature Space (by Client Type)')
        axes[0].set_xlabel('Dimension 1')
        axes[0].set_ylabel('Dimension 2')
        
        # æ·»åŠ å›¾ä¾‹
        red_patch = plt.matplotlib.patches.Patch(color='red', label='Malicious')
        blue_patch = plt.matplotlib.patches.Patch(color='blue', label='Benign')
        axes[0].legend(handles=[red_patch, blue_patch])
        
        # 2. æŒ‰èšç±»ç»“æœç€è‰²
        cluster_colors = ['orange', 'green']
        for i in range(len(features_2d)):
            axes[1].scatter(features_2d[i, 0], features_2d[i, 1], 
                           c=cluster_colors[labels[i]], alpha=0.7, s=100)
        axes[1].set_title('LFighter: Feature Space (by Cluster)')
        axes[1].set_xlabel('Dimension 1')
        axes[1].set_ylabel('Dimension 2')
        
        # æ·»åŠ èšç±»å›¾ä¾‹
        orange_patch = plt.matplotlib.patches.Patch(color='orange', label='Cluster 0')
        green_patch = plt.matplotlib.patches.Patch(color='green', label='Cluster 1')
        axes[1].legend(handles=[orange_patch, green_patch])
        
        plt.tight_layout()
        plt.savefig(f'{self.save_path}/lfighter_feature_space_round_{round_num}.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def visualize_cluster_quality(self, cs0, cs1, good_cl, round_num):
        """å¯è§†åŒ–èšç±»è´¨é‡"""
        if not self.should_visualize_this_round():
            return
            
        # åªåœ¨PDFæŠ¥å‘Šä¸­æ·»åŠ èšç±»è´¨é‡å¯è§†åŒ–ï¼Œä¸ç”Ÿæˆå•ç‹¬çš„å›¾ç‰‡æ–‡ä»¶
        if self.save_as_pdf and self.pdf_pages is not None:
            plt.figure(figsize=(10, 6))
            clusters = ['Cluster 0', 'Cluster 1']
            dissimilarities = [cs0, cs1]
            colors = ['green' if i == good_cl else 'red' for i in range(2)]
            
            plt.bar(clusters, dissimilarities, color=colors, alpha=0.7)
            plt.ylabel('Dissimilarity Score')
            plt.title('Cluster Quality Comparison')
            plt.grid(True, alpha=0.3)
            
            # æ·»åŠ å›¾ä¾‹
            green_patch = plt.matplotlib.patches.Patch(color='green', label='Good Cluster')
            red_patch = plt.matplotlib.patches.Patch(color='red', label='Bad Cluster')
            plt.legend(handles=[green_patch, red_patch])
            
            try:
                self.pdf_pages.savefig(plt.gcf(), bbox_inches='tight')
            except Exception as e:
                print(f"[LFighter-AE] èšç±»è´¨é‡å¯è§†åŒ–æ·»åŠ åˆ°PDFå¤±è´¥: {e}")
            plt.close()
    
    def visualize_client_scores(self, scores, ptypes, round_num):
        """å¯è§†åŒ–å®¢æˆ·ç«¯å¾—åˆ†"""
        if not self.should_visualize_this_round():
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        client_indices = range(len(scores))
        
        # å¢å¼ºçš„å®¢æˆ·ç«¯ç±»å‹æ£€æµ‹é€»è¾‘
        colors = []
        malicious_count = 0
        for ptype in ptypes:
            ptype_str = str(ptype).lower()
            if ('malicious' in ptype_str or 'attack' in ptype_str or 
                'bad' in ptype_str or 'adversarial' in ptype_str):
                colors.append('red')
                malicious_count += 1
            else:
                colors.append('blue')
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ£€æµ‹ç»“æœ
        if self.enable_visualization and round_num == 1:
            print(f"[LFighter Client Scores] Detected {malicious_count}/{len(ptypes)} malicious clients")
        
        # 1. å®¢æˆ·ç«¯å¾—åˆ†æ¡å½¢å›¾
        bars = axes[0].bar(client_indices, scores, color=colors, alpha=0.7)
        axes[0].set_title('LFighter: Client Scores')
        axes[0].set_xlabel('Client Index')
        axes[0, 1].set_ylim(0, 1.1)
        axes[0].grid(True, alpha=0.3)
        
        # æ·»åŠ å›¾ä¾‹
        red_patch = plt.matplotlib.patches.Patch(color='red', label='Malicious')
        blue_patch = plt.matplotlib.patches.Patch(color='blue', label='Benign')
        axes[0].legend(handles=[red_patch, blue_patch])
        
        # 2. å¾—åˆ†åˆ†å¸ƒç›´æ–¹å›¾
        axes[1].hist(scores, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
        axes[1].axvline(np.mean(scores), color='red', linestyle='--', 
                       label=f'Mean: {np.mean(scores):.4f}')
        axes[1].axvline(np.median(scores), color='orange', linestyle='--', 
                       label=f'Median: {np.median(scores):.4f}')
        axes[1].set_xlabel('Score')
        axes[1].set_ylabel('Frequency')
        axes[1].set_title('LFighter: Score Distribution')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.save_path}/lfighter_client_scores_round_{round_num}.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_summary_report(self, round_num, metrics):
        """åˆ›å»ºæ€»ç»“æŠ¥å‘Š"""
        if not self.should_visualize_this_round():
            return
        
        # åªåœ¨PDFæŠ¥å‘Šä¸­æ·»åŠ æ€»ç»“æŠ¥å‘Šï¼Œä¸ç”Ÿæˆå•ç‹¬çš„æ–‡æœ¬æ–‡ä»¶
        if self.save_as_pdf and self.pdf_pages is not None:
            # åˆ›å»ºæ–‡æœ¬æŠ¥å‘Š
            report_content = f"""
LFighter-Autoencoder Algorithm Summary Report
==========================================
Round: {round_num}
Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}

Algorithm Parameters:
- Hidden Dimension: {self.ae_hidden_dim}
- Latent Dimension: {self.ae_latent_dim}
- Training Epochs: {self.ae_epochs}
- Reconstruction Weight: {self.reconstruction_weight}

Performance Metrics:
- Total Clients: {metrics.get('total_clients', 'N/A')}
- Good Clients Selected: {metrics.get('good_clients', 'N/A')}
- Selection Accuracy: {metrics.get('accuracy', 'N/A'):.2%}
- Cluster 0 Dissimilarity: {metrics.get('cs0', 'N/A'):.4f}
- Cluster 1 Dissimilarity: {metrics.get('cs1', 'N/A'):.4f}
- Good Cluster: {metrics.get('good_cluster', 'N/A')}
- Mean Reconstruction Error: {metrics.get('mean_recon_error', 'N/A'):.4f}
- Std Reconstruction Error: {metrics.get('std_recon_error', 'N/A'):.4f}

Feature Processing:
- Feature Strategy: {metrics.get('feature_strategy', 'ä½¿ç”¨å…¨éƒ¨è¾“å‡ºå±‚æ¢¯åº¦')}
- Original Feature Dimension: {metrics.get('feature_dim', 'N/A')}
- Dimension Reduction Method: {metrics.get('reduction_method', 'N/A')}
- Final Loss: {metrics.get('final_loss', 'N/A'):.6f}

Attack Analysis:
- Attack Type: {metrics.get('attack_type', 'N/A')}
- Attack Scope: {metrics.get('attack_scope', 'N/A')}
- Attack Description: {metrics.get('attack_description', 'N/A')}
- Confidence: {metrics.get('attack_confidence', 'N/A'):.2f}
"""
            
            # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬å›¾åƒï¼Œæ·»åŠ åˆ°PDF
            fig = plt.figure(figsize=(12, 10))
            plt.text(0.1, 0.5, report_content, fontsize=10, family='monospace')
            plt.axis('off')
            
            try:
                self.pdf_pages.savefig(fig, bbox_inches='tight')
            except Exception as e:
                print(f"[LFighter-AE] æ€»ç»“æŠ¥å‘Šæ·»åŠ åˆ°PDFå¤±è´¥: {e}")
            plt.close()
    
    def clusters_dissimilarity(self, clusters):
        """è®¡ç®—èšç±»é—´ç›¸å¼‚æ€§ï¼Œå¤„ç†ç©ºèšç±»æƒ…å†µ"""
        n0 = len(clusters[0])
        n1 = len(clusters[1])
        m = n0 + n1 
        
        # å¤„ç†ç©ºèšç±»æƒ…å†µ
        if n0 == 0:
            return 1.0, 0.0  # ç©ºèšç±»0ï¼Œèšç±»1æ›´å¥½
        if n1 == 0:
            return 0.0, 1.0  # ç©ºèšç±»1ï¼Œèšç±»0æ›´å¥½
        if n0 == 1:
            ds0 = 1.0  # å•æ ·æœ¬èšç±»è´¨é‡è®¾ä¸ºæœ€å·®
        else:
            cs0 = smp.cosine_similarity(clusters[0]) - np.eye(n0)
            mincs0 = np.min(cs0, axis=1)
            ds0 = n0/m * (1 - np.mean(mincs0))
        
        if n1 == 1:
            ds1 = 1.0  # å•æ ·æœ¬èšç±»è´¨é‡è®¾ä¸ºæœ€å·®
        else:
            cs1 = smp.cosine_similarity(clusters[1]) - np.eye(n1)
            mincs1 = np.min(cs1, axis=1)
            ds1 = n1/m * (1 - np.mean(mincs1))
        
        return ds0, ds1

    def aggregate(self, global_model, local_models, ptypes):
        local_weights = [copy.deepcopy(model).state_dict() for model in local_models]
        m = len(local_models)
        
        # å¯è§†åŒ–æ¨¡å¼ä¸‹å¢åŠ è½®æ•°è®¡æ•°
        if self.enable_visualization:
            self.round_counter += 1
            # æç¤ºå½“å‰å¯è§†åŒ–é¢‘ç‡
            current_freq = 1 if self.round_counter <= 20 else 10
            next_viz_round = self.round_counter if self.round_counter % current_freq == 0 else ((self.round_counter // current_freq) + 1) * current_freq
            print(f"[LFighter] Round {self.round_counter} - Visualization freq: every {current_freq} rounds (next: round {next_viz_round})")
        
        for i in range(m):
            local_models[i] = list(local_models[i].parameters())
        global_model = list(global_model.parameters())
        dw = [None for i in range(m)]
        db = [None for i in range(m)]
        for i in range(m):
            dw[i]= global_model[-2].cpu().data.numpy() - \
                local_models[i][-2].cpu().data.numpy() 
            db[i]= global_model[-1].cpu().data.numpy() - \
                local_models[i][-1].cpu().data.numpy()
        dw = np.asarray(dw)
        db = np.asarray(db)

        "If one class or two classes classification model"
        if len(db[0]) <= 2:
            data = []
            for i in range(m):
                data.append(dw[i].reshape(-1))
        
            kmeans = KMeans(n_clusters=2, random_state=0).fit(data)
            labels = kmeans.labels_

            clusters = {0:[], 1:[]}
            for i, l in enumerate(labels):
                clusters[l].append(data[i])

            good_cl = 0
            cs0, cs1 = self.clusters_dissimilarity(clusters)
            if cs0 < cs1:
                good_cl = 1

            scores = np.ones([m])
            for i, l in enumerate(labels):
                if l != good_cl:
                    scores[i] = 0
            
            # å¯è§†åŒ–ç»“æœ
            if self.enable_visualization:
                print(f'[LFighter] Cluster quality: cs0={cs0:.4f}, cs1={cs1:.4f}, good_cluster={good_cl}')
                print(f'[LFighter] Selected good clients: {np.sum(scores)}/{m}')
                
                if self.should_visualize_this_round():
                    # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
                    metrics = {
                        'total_clients': m,
                        'good_clients': int(np.sum(scores)),
                        'accuracy': int(np.sum(scores)) / m,
                        'cs0': cs0,
                        'cs1': cs1,
                        'good_cluster': good_cl,
                        'reduction_method': 'Binary classification',
                        'anomalous_classes': 'N/A (Binary)'
                    }
                    
                    # ç”ŸæˆPDFæŠ¥å‘Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    self.create_pdf_report(self.round_counter, np.array(data), labels, ptypes, scores, cs0, cs1, good_cl, metrics)
                    
                    # ç”Ÿæˆå•ç‹¬çš„PNGæ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if self.keep_individual_files:
                        self.visualize_feature_space(np.array(data), labels, ptypes, self.round_counter)
                        self.visualize_cluster_quality(cs0, cs1, good_cl, self.round_counter)
                        self.visualize_client_scores(scores, ptypes, self.round_counter)
                        self.create_summary_report(self.round_counter, metrics)
                    
                    # æ¸…ç†æ—§çš„å¯è§†åŒ–æ–‡ä»¶
                    self.cleanup_old_visualizations()
                
            global_weights = average_weights(local_weights, scores)
            return global_weights

        "For multiclassification models"
        norms = np.linalg.norm(dw, axis = -1) 
        self.memory = np.sum(norms, axis = 0)
        self.memory +=np.sum(abs(db), axis = 0)
        max_two_freq_classes = self.memory.argsort()[-2:]
        print('Potential source and target classes:', max_two_freq_classes)
        data = []
        for i in range(m):
            data.append(dw[i][max_two_freq_classes].reshape(-1))

        # === ç»Ÿä¸€é™ç»´å¤„ç†ï¼šç¡®ä¿æ¶ˆèå®éªŒå…¬å¹³æ€§ ===
        def unified_dimension_reduction(features, target_dim=200, method='auto', standardize=True):
            """ç»Ÿä¸€é™ç»´å‡½æ•°ï¼šä¸å¤šè§†å›¾ç®—æ³•ä¿æŒä¸€è‡´"""
            features_array = np.array(features)
            n_samples, original_dim = features_array.shape
            
            # 1. æ ‡å‡†åŒ–å¤„ç†
            if standardize:
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                features_array = scaler.fit_transform(features_array)
                std_info = "standardized"
            else:
                std_info = "raw"
            
            # 2. æ™ºèƒ½é™ç»´ç­–ç•¥
            max_pca_components = min(n_samples, original_dim)
            effective_target_dim = min(target_dim, max_pca_components)
            
            if original_dim <= effective_target_dim:
                return features_array, f"keep_all_{original_dim}_{std_info}"
            elif original_dim <= effective_target_dim * 2:
                return features_array[:, :effective_target_dim], f"truncate_{effective_target_dim}_{std_info}"
            else:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=effective_target_dim, random_state=42)
                reduced_features = pca.fit_transform(features_array)
                explained_ratio = np.sum(pca.explained_variance_ratio_)
                return reduced_features, f"pca_{effective_target_dim}_var{explained_ratio:.3f}_{std_info}"
        
        # åº”ç”¨ç»Ÿä¸€é™ç»´ï¼ˆä¸å¤šè§†å›¾ç®—æ³•ä¿æŒä¸€è‡´ï¼‰
        data_reduced, reduction_method = unified_dimension_reduction(data, target_dim=200)
        print(f'[LFighter] Applied unified dimension reduction: {reduction_method}')
        
        kmeans = KMeans(n_clusters=2, random_state=0).fit(data_reduced)
        labels = kmeans.labels_

        clusters = {0:[], 1:[]}
        for i, l in enumerate(labels):
          clusters[l].append(data_reduced[i])

        good_cl = 0
        cs0, cs1 = self.clusters_dissimilarity(clusters)
        if cs0 < cs1:
            good_cl = 1

        scores = np.ones([m])
        for i, l in enumerate(labels):
            if l != good_cl:
                scores[i] = 0
        
        # å¯è§†åŒ–ç»“æœ
        if self.enable_visualization:
            print(f'[LFighter] Cluster quality: cs0={cs0:.4f}, cs1={cs1:.4f}, good_cluster={good_cl}')
            print(f'[LFighter] Selected good clients: {np.sum(scores)}/{m}')
            
            if self.should_visualize_this_round():
                # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
                metrics = {
                    'total_clients': m,
                    'good_clients': int(np.sum(scores)),
                    'accuracy': int(np.sum(scores)) / m,
                    'cs0': cs0,
                    'cs1': cs1,
                    'good_cluster': good_cl,
                    'reduction_method': reduction_method,
                    'anomalous_classes': str(max_two_freq_classes)
                }
                
                # ç”ŸæˆPDFæŠ¥å‘Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
                self.create_pdf_report(self.round_counter, data_reduced, labels, ptypes, scores, cs0, cs1, good_cl, metrics)
                
                # ç”Ÿæˆå•ç‹¬çš„PNGæ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if self.keep_individual_files:
                    self.visualize_feature_space(data_reduced, labels, ptypes, self.round_counter)
                    self.visualize_cluster_quality(cs0, cs1, good_cl, self.round_counter)
                    self.visualize_client_scores(scores, ptypes, self.round_counter)
                    self.create_summary_report(self.round_counter, metrics)
                
                # æ¸…ç†æ—§çš„å¯è§†åŒ–æ–‡ä»¶
                self.cleanup_old_visualizations()
            
        global_weights = average_weights(local_weights, scores)
        return global_weights


################################################
# Takes in grad
# Compute similarity
# Get weightings
def foolsgold(grads):
    n_clients = grads.shape[0]
    cs = smp.cosine_similarity(grads) - np.eye(n_clients)
    maxcs = np.max(cs, axis=1)
    # pardoning
    for i in range(n_clients):
        for j in range(n_clients):
            if i == j:
                continue
            if maxcs[i] < maxcs[j]:
                cs[i][j] = cs[i][j] * maxcs[i] / maxcs[j]
    wv = 1 - (np.max(cs, axis=1))
    wv[wv > 1] = 1
    wv[wv < 0] = 0

    # Rescale so that max value is wv
    wv = wv / np.max(wv)
    wv[(wv == 1)] = .99

    # Logit function
    wv = (np.log(wv / (1 - wv)) + 0.5)
    wv[(np.isinf(wv) + wv > 1)] = 1
    wv[(wv < 0)] = 0

    return wv


class FoolsGold:
    def __init__(self, num_peers):
        self.memory = None
        self.wv_history = []
        self.num_peers = num_peers
       
    def score_gradients(self, local_grads, selectec_peers):
        m = len(local_grads)
        grad_len = np.array(local_grads[0][-2].cpu().data.numpy().shape).prod()
        if self.memory is None:
            self.memory = np.zeros((self.num_peers, grad_len))

        grads = np.zeros((m, grad_len))
        for i in range(m):
            grads[i] = np.reshape(local_grads[i][-2].cpu().data.numpy(), (grad_len))
        self.memory[selectec_peers]+= grads
        wv = foolsgold(self.memory)  # Use FG
        self.wv_history.append(wv)
        return wv[selectec_peers]


#######################################################################################



class LFighterDBO:
    def __init__(self):
        pass
    
    def aggregate(self, simulation_model, local_weights, local_features=None):
        import config
        device = simulation_model.device if hasattr(simulation_model, 'device') else config.DEVICE
        
        import torch
        from models import DBONet
        from sklearn.neighbors import kneighbors_graph
        
        # ç›´æ¥åœ¨è¿™é‡Œå®šä¹‰å½’ä¸€åŒ–å‡½æ•°ï¼Œé¿å…å¾ªç¯å¯¼å…¥
        def normalization(data):
            """æ•°æ®å½’ä¸€åŒ–å‡½æ•°"""
            maxVal = torch.max(data)
            minVal = torch.min(data)
            data = (data - minVal) / (maxVal - minVal + 1e-10)  # é¿å…é™¤é›¶
            return data

        def standardization(data):
            """æ•°æ®æ ‡å‡†åŒ–å‡½æ•°"""
            rowSum = torch.sqrt(torch.sum(data**2, 1))
            repMat = rowSum.repeat((data.shape[1], 1)) + 1e-10
            data = torch.div(data, repMat.t())
            return data
        
        m = len(local_weights)
        print(f"[LFighter-DBO] Processing {m} clients with lightweight DBONet")
        
        # åªæå–è¾“å‡ºå±‚æ¢¯åº¦ç‰¹å¾
        local_models = []
        for local_weight in local_weights:
            model = copy.deepcopy(simulation_model)
            model.load_state_dict(local_weight)
            local_models.append(list(model.parameters()))
        
        global_model = list(simulation_model.parameters())
        
        # ä½¿ç”¨ä¸LFighterä¸€è‡´çš„å…³é”®ç±»åˆ«æ£€æµ‹å’Œæå–
        # é¦–å…ˆæ£€æµ‹æœ€å¼‚å¸¸çš„ä¸¤ä¸ªç±»åˆ«ï¼ˆä¸LFighteré€»è¾‘å®Œå…¨ä¸€è‡´ï¼‰
        memory = np.zeros(global_model[-1].shape[0])  # è¾“å‡ºå±‚åç½®çš„ç»´åº¦
        
        for i in range(m):
            # è®¡ç®—æƒé‡å’Œåç½®çš„å·®å¼‚
            dw = global_model[-2].cpu().data.numpy() - local_models[i][-2].cpu().data.numpy()
            db = global_model[-1].cpu().data.numpy() - local_models[i][-1].cpu().data.numpy()
            
            # ç´¯ç§¯å¼‚å¸¸ç¨‹åº¦ï¼ˆä¸åŸç‰ˆLFighterçš„é€»è¾‘ä¸€è‡´ï¼‰
            norms = np.linalg.norm(dw, axis=-1)
            memory += norms + np.abs(db)
        
        # æ‰¾åˆ°æœ€å¼‚å¸¸çš„ä¸¤ä¸ªç±»åˆ«
        max_two_freq_classes = memory.argsort()[-2:]
        print(f'[LFighter-DBO] Detected anomalous classes (same as LFighter): {max_two_freq_classes}')
        
        # åªæå–è¿™ä¸¤ä¸ªç±»åˆ«çš„è¾“å‡ºå±‚æ¢¯åº¦ï¼ˆä¸LFighterå®Œå…¨ä¸€è‡´ï¼‰
        gradients = []
        for i in range(m):
            dw = global_model[-2].cpu().data.numpy() - local_models[i][-2].cpu().data.numpy()
            key_classes_grad = dw[max_two_freq_classes].reshape(-1)
            gradients.append(key_classes_grad)
        
        feature_matrix = np.array(gradients)
        print(f"[LFighter-DBO] Gradient matrix shape: {feature_matrix.shape}")
        
        # === ç»Ÿä¸€é™ç»´å¤„ç†ï¼šç¡®ä¿æ¶ˆèå®éªŒå…¬å¹³æ€§ ===
        def unified_dimension_reduction(features, target_dim=200, method='auto', standardize=True):
            """ç»Ÿä¸€é™ç»´å‡½æ•°ï¼šä¸å¤šè§†å›¾ç®—æ³•ä¿æŒä¸€è‡´"""
            features_array = np.array(features)
            n_samples, original_dim = features_array.shape
            
            # 1. æ ‡å‡†åŒ–å¤„ç†
            if standardize:
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                features_array = scaler.fit_transform(features_array)
                std_info = "standardized"
            else:
                std_info = "raw"
            
            # 2. æ™ºèƒ½é™ç»´ç­–ç•¥
            max_pca_components = min(n_samples, original_dim)
            effective_target_dim = min(target_dim, max_pca_components)
            
            if original_dim <= effective_target_dim:
                return features_array, f"keep_all_{original_dim}_{std_info}"
            elif original_dim <= effective_target_dim * 2:
                return features_array[:, :effective_target_dim], f"truncate_{effective_target_dim}_{std_info}"
            else:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=effective_target_dim, random_state=42)
                reduced_features = pca.fit_transform(features_array)
                explained_ratio = np.sum(pca.explained_variance_ratio_)
                return reduced_features, f"pca_{effective_target_dim}_var{explained_ratio:.3f}_{std_info}"
        
        # åº”ç”¨ç»Ÿä¸€é™ç»´ï¼ˆä¸å¤šè§†å›¾ç®—æ³•ä¿æŒä¸€è‡´ï¼‰
        feature_matrix_reduced, reduction_method = unified_dimension_reduction(feature_matrix, target_dim=200)
        print(f"[LFighter-DBO] Applied unified dimension reduction: {reduction_method}")
        
        # è½»é‡çº§DBONeté…ç½®ï¼šblocks=1-2ï¼Œå¿«é€Ÿè®­ç»ƒ
        n_view = 1
        nfeats = [feature_matrix_reduced.shape[1]]  # ä½¿ç”¨é™ç»´åçš„ç»´åº¦
        n_clusters = 2
        blocks = 2  # ç»Ÿä¸€é…ç½®
        para = 0.05  # ç»Ÿä¸€å‚æ•°ï¼ˆä¸MV-DBOä¿æŒä¸€è‡´ï¼‰
        np.random.seed(42)
        Z_init = np.random.randn(m, n_clusters) * 0.01  # ç»Ÿä¸€åˆå§‹åŒ–ç¼©æ”¾
        
        # ç»Ÿä¸€é‚»æ¥çŸ©é˜µé…ç½®
        n_neighbors = min(5, m-1)  # ä¸MV-DBOä¿æŒä¸€è‡´çš„é‚»å±…æ•°é‡
        adj = kneighbors_graph(feature_matrix_reduced, n_neighbors=n_neighbors, mode='connectivity', include_self=True)
        adj_tensor = torch.tensor(adj.toarray(), dtype=torch.float32, device=device)
        
        # åˆ›å»ºè½»é‡çº§DBONet
        dbo_model = DBONet(nfeats, n_view, n_clusters, blocks, para, Z_init, device)
        features_tensor = [torch.tensor(feature_matrix_reduced, dtype=torch.float32, device=device)]
        adjs = [adj_tensor]
        
        # ç»Ÿä¸€å½’ä¸€åŒ–æµç¨‹ï¼ˆä¸MV-DBOä¸€è‡´ï¼‰
        features_norm = []
        for i in range(n_view):
            # å¤šæ­¥å½’ä¸€åŒ–
            feature = features_tensor[i]
            feature = (feature - feature.mean(dim=0)) / (feature.std(dim=0) + 1e-8)  # æ ‡å‡†åŒ–
            feature = standardization(normalization(feature))  # è¿›ä¸€æ­¥å½’ä¸€åŒ–
            features_norm.append(feature)
        
        # ç»Ÿä¸€è®­ç»ƒé…ç½®ï¼šä¸MV-DBOå®Œå…¨ç›¸åŒ
        dbo_model.train()
        optimizer = torch.optim.Adam(dbo_model.parameters(), lr=5e-4, weight_decay=1e-5)  # ç»Ÿä¸€ä¼˜åŒ–å™¨é…ç½®
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)  # ç»Ÿä¸€è°ƒåº¦å™¨
        criterion = torch.nn.MSELoss()
        
        best_loss = float('inf')
        for epoch in range(8):  # ç»Ÿä¸€è®­ç»ƒè½®æ•°ï¼š8ä¸ªepoch
            optimizer.zero_grad()
            output_z = dbo_model(features_tensor, adjs)
            
            # ç»Ÿä¸€æŸå¤±å‡½æ•°ï¼šæ˜¾å¼+éšå¼æŸå¤±ï¼ˆä¸MV-DBOä¸€è‡´ï¼‰
            loss_dis = torch.tensor(0., device=device)
            loss_lap = torch.tensor(0., device=device)
            
            for k in range(n_view):
                # æ˜¾å¼æŸå¤±ï¼šç‰¹å¾é‡æ„
                target_sim = features_norm[k] @ features_norm[k].t()
                pred_sim = output_z @ output_z.t()
                loss_dis += criterion(pred_sim, target_sim)
                
                # éšå¼æŸå¤±ï¼šå›¾æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–
                loss_lap += criterion(pred_sim, adjs[k])
            
            # ç»Ÿä¸€æ€»æŸå¤±æƒé‡
            total_loss = loss_dis + 0.3 * loss_lap
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(dbo_model.parameters(), max_norm=1.0)  # ç»Ÿä¸€æ¢¯åº¦è£å‰ª
            optimizer.step()
            scheduler.step()
            
            if total_loss.item() < best_loss:
                best_loss = total_loss.item()
            
            if epoch % 2 == 0:
                print(f"[LFighter-DBO] Epoch {epoch+1}: Loss={total_loss.item():.6f}")
        
        # === ä½¿ç”¨LFD clusters_dissimilarityæ›¿æ¢Silhouetteè¯„ä¼° ===
        dbo_model.eval()
        with torch.no_grad():
            output_z = dbo_model(features_tensor, adjs)
        
        z_np = output_z.detach().cpu().numpy()
        
        # ç®€åŒ–çš„kmeansèšç±»ï¼ˆä¸LFighter-MV-DBOä¸€è‡´ï¼‰
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10).fit(z_np)
        labels = kmeans.labels_
        
        # === ä½¿ç”¨LFDçš„clusters_dissimilarityè¿›è¡Œè´¨é‡è¯„ä¼° ===
        clusters = {0: [], 1: []}
        for i, l in enumerate(labels):
            clusters[l].append(z_np[i])
        
        # åˆ›å»ºLFDå®ä¾‹å¹¶è°ƒç”¨clusters_dissimilarityæ–¹æ³•
        lfd = LFD(config.NUM_CLASSES)
        cs0, cs1 = lfd.clusters_dissimilarity(clusters)
        good_cl = 0 if cs0 < cs1 else 1
        
        print(f"[LFighter-DBO] LFD cluster quality: cs0={cs0:.4f}, cs1={cs1:.4f}, good_cluster={good_cl}")
        
        # æƒé‡åˆ†é…
        scores = np.ones(m)
        for i, l in enumerate(labels):
            if l != good_cl:
                scores[i] = 0
        
        print(f"[LFighter-DBO] Good clients: {np.sum(scores)}/{m}")
        return average_weights(local_weights, scores)


class LFighterMV:
    def __init__(self):
        pass
    
    def aggregate(self, simulation_model, local_weights, local_features, local_models, peers_types, lfd):
        """LFighter-MV: ä¸‰è§†å›¾æ‹¼æ¥ - è¾“å‡ºå±‚æ¢¯åº¦ï¼Œç¬¬ä¸€å±‚æ¿€æ´»å€¼ï¼Œè¾“å…¥å±‚æ¢¯åº¦"""
        # ä¸¥æ ¼æ£€æŸ¥ï¼šç¼ºå°‘ç‰¹å¾æ—¶ç›´æ¥æŠ¥é”™
        if not local_features or len(local_features) == 0:
            raise ValueError("LFighter-MV requires local_features but got None or empty list. Please check if 'mv' rule is properly extracting features during training.")
        
        m = len(local_weights)
        
        # è§†å›¾1: è¾“å‡ºå±‚æ¢¯åº¦ï¼ˆæƒé‡å·®å¼‚ï¼‰- ä¸åŸç‰ˆLFighterä¿æŒä¸€è‡´
        output_grad_features = []
        
        # é¦–å…ˆè®¡ç®—æ‰€æœ‰ç±»åˆ«çš„æ¢¯åº¦å·®å¼‚æ¥æ£€æµ‹æœ€å¼‚å¸¸çš„ä¸¤ä¸ªç±»åˆ«ï¼ˆä¸LFighterä¿æŒä¸€è‡´ï¼‰
        global_params = list(simulation_model.parameters())
        
        # è®¡ç®—æ‰€æœ‰å®¢æˆ·ç«¯çš„è¾“å‡ºå±‚åç½®å·®å¼‚ï¼Œç”¨äºæ£€æµ‹å¼‚å¸¸ç±»åˆ«
        memory = np.zeros(global_params[-1].shape[0])  # è¾“å‡ºå±‚åç½®çš„ç»´åº¦
        
        for i in range(m):
            local_model = copy.deepcopy(simulation_model)
            local_model.load_state_dict(local_weights[i])
            local_params = list(local_model.parameters())
            
            # è®¡ç®—æƒé‡å’Œåç½®çš„å·®å¼‚
            dw = global_params[-2].cpu().data.numpy() - local_params[-2].cpu().data.numpy()
            db = global_params[-1].cpu().data.numpy() - local_params[-1].cpu().data.numpy()
            
            # ç´¯ç§¯å¼‚å¸¸ç¨‹åº¦ï¼ˆä¸åŸç‰ˆLFighterçš„é€»è¾‘ä¸€è‡´ï¼‰
            norms = np.linalg.norm(dw, axis=-1)
            memory += norms + np.abs(db)
        
        # æ‰¾åˆ°æœ€å¼‚å¸¸çš„ä¸¤ä¸ªç±»åˆ«ï¼ˆä¸åŸç‰ˆLFighterä¸€è‡´ï¼‰
        max_two_freq_classes = memory.argsort()[-2:]
        print(f'[LFighter-MV] Detected anomalous classes (same as LFighter): {max_two_freq_classes}')
        
        # ç°åœ¨åªæå–è¿™ä¸¤ä¸ªç±»åˆ«çš„è¾“å‡ºå±‚æ¢¯åº¦
        for i in range(m):
            local_model = copy.deepcopy(simulation_model)
            local_model.load_state_dict(local_weights[i])
            local_params = list(local_model.parameters())
            
            # åªæå–æœ€å¼‚å¸¸ä¸¤ä¸ªç±»åˆ«çš„æƒé‡å·®å¼‚ï¼ˆä¸LFighterå®Œå…¨ä¸€è‡´ï¼‰
            output_grad = global_params[-2].cpu().data.numpy() - local_params[-2].cpu().data.numpy()
            key_classes_grad = output_grad[max_two_freq_classes].reshape(-1)
            output_grad_features.append(key_classes_grad)
        
        # è§†å›¾2: ç¬¬ä¸€å±‚æ¿€æ´»å€¼ï¼ˆä»local_featuresä¸­æå–ï¼‰
        first_activation_features = []
        for i, peer_features in enumerate(local_features):
            if not peer_features or len(peer_features) == 0:
                raise ValueError(f"Empty peer_features for peer {i}. Check feature extraction during training.")
            
            # è°ƒè¯•ï¼šæ‰“å°å®é™…çš„æ•°æ®ç»“æ„
            print(f"[Debug] Peer {i} features structure: type={type(peer_features)}, len={len(peer_features) if hasattr(peer_features, '__len__') else 'N/A'}")
            if hasattr(peer_features, '__len__') and len(peer_features) > 0:
                print(f"[Debug] First element type: {type(peer_features[0])}")
                if hasattr(peer_features[0], '__len__'):
                    print(f"[Debug] First element length: {len(peer_features[0])}")
            
            # å¤„ç†å¯èƒ½çš„åµŒå¥—ç»“æ„
            # peer_featuresåº”è¯¥æ˜¯[input_flat, first_layer_activation, logits]
            # ä½†å¦‚æœæ˜¯åµŒå¥—çš„ï¼Œå¯èƒ½éœ€è¦é¢å¤–å¤„ç†
            actual_features = peer_features
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åµŒå¥—ç»“æ„ï¼ˆlist of listsï¼‰
            if isinstance(peer_features, (list, tuple)) and len(peer_features) > 0:
                if isinstance(peer_features[0], (list, tuple)):
                    # å¦‚æœæ˜¯åµŒå¥—çš„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆå‡è®¾å®ƒæ˜¯æˆ‘ä»¬éœ€è¦çš„ç‰¹å¾ï¼‰
                    actual_features = peer_features[0]
                    print(f"[Debug] Detected nested structure, using first element")
            
            # ç°åœ¨æ£€æŸ¥actual_featuresæ˜¯å¦ç¬¦åˆé¢„æœŸæ ¼å¼
            if not isinstance(actual_features, (list, tuple)) or len(actual_features) < 2:
                raise ValueError(f"Invalid features structure for peer {i}: expected list/tuple with >=2 elements, got {type(actual_features)} with length {len(actual_features) if hasattr(actual_features, '__len__') else 'unknown'}")
            
            # ä»CNNPATHMNISTçš„return_featuresä¸­æå–ç¬¬ä¸€å±‚æ¿€æ´»å€¼ï¼ˆç´¢å¼•1ï¼‰
            first_activation = actual_features[1]  # first_layer_activation
            print(f"[Debug] Peer {i} first_activation type: {type(first_activation)}")
            
            if not hasattr(first_activation, 'detach'):
                # å¦‚æœfirst_activationä¸æ˜¯tensorï¼Œå°è¯•è½¬æ¢
                if isinstance(first_activation, (list, tuple)):
                    # å¦‚æœæ˜¯list/tupleï¼Œå¯èƒ½åŒ…å«å¤šä¸ªtensorï¼Œå–ç¬¬ä¸€ä¸ª
                    if len(first_activation) > 0 and hasattr(first_activation[0], 'detach'):
                        first_activation = first_activation[0]
                        print(f"[Debug] Converted from list to tensor for peer {i}")
                    else:
                        raise ValueError(f"Cannot convert first_activation to tensor for peer {i}: {type(first_activation)}")
                else:
                    raise ValueError(f"Expected tensor with .detach() method for peer {i}, got {type(first_activation)}")
            
            # åªå–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ¿€æ´»å€¼ä½œä¸ºä»£è¡¨
            first_activation_flat = first_activation[0].detach().cpu().numpy().flatten()
            first_activation_features.append(first_activation_flat)
            print(f"[Debug] Successfully extracted features for peer {i}, shape: {first_activation_flat.shape}")
        
        # è§†å›¾3: è¾“å…¥å±‚æ¢¯åº¦ï¼ˆç¬¬ä¸€å±‚å·ç§¯æƒé‡çš„æ¢¯åº¦ï¼‰
        input_grad_features = []
        for i in range(m):
            # è·å–ç¬¬ä¸€å±‚å·ç§¯å±‚çš„æƒé‡æ¢¯åº¦ï¼ˆconv1æƒé‡ï¼‰
            input_grad = list(simulation_model.parameters())[0].cpu().data.numpy() - list(local_models[i].parameters())[0].cpu().data.numpy()
            input_grad_features.append(input_grad.reshape(-1))
        
        # æ™ºèƒ½ç»´åº¦å¤„ç†ï¼šæ ¹æ®ç‰¹å¾ç»´åº¦é€‰æ‹©è£å‰ªæˆ–PCAé™ç»´ç­–ç•¥
        def smart_dimension_reduction(features, target_dim=200, method='auto', standardize=True):
            """æ™ºèƒ½ç»´åº¦ç¼©å‡ï¼šæ ‡å‡†åŒ– + PCA/è£å‰ªï¼Œè€ƒè™‘æ ·æœ¬æ•°é™åˆ¶"""
            features_array = np.array(features)
            n_samples, original_dim = features_array.shape
            
            # 1. å¼ºçƒˆå»ºè®®çš„æ ‡å‡†åŒ–å¤„ç†ï¼ˆå‡å€¼0ï¼Œæ–¹å·®1ï¼‰
            if standardize:
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                features_array = scaler.fit_transform(features_array)
                std_info = "standardized"
            else:
                std_info = "raw"
            
            # 2. PCAæœ€å¤§ç»´åº¦ä¸èƒ½è¶…è¿‡min(æ ·æœ¬æ•°, ç‰¹å¾æ•°)
            max_pca_components = min(n_samples, original_dim)
            effective_target_dim = min(target_dim, max_pca_components)
            
            if original_dim <= effective_target_dim:
                # ç»´åº¦å·²ç»è¶³å¤Ÿå°ï¼Œç›´æ¥è¿”å›æ ‡å‡†åŒ–åçš„ç‰¹å¾
                return features_array, f"keep_all_{original_dim}_{std_info}"
            elif original_dim <= effective_target_dim * 2:
                # ç»´åº¦é€‚ä¸­ï¼Œä½¿ç”¨ç®€å•è£å‰ªï¼ˆé€Ÿåº¦ä¼˜å…ˆï¼‰
                return features_array[:, :effective_target_dim], f"truncate_{effective_target_dim}_{std_info}"
            else:
                # ç»´åº¦å¾ˆå¤§ï¼Œä½¿ç”¨PCAé™ç»´ï¼ˆä¿¡æ¯ä¿ç•™ä¼˜å…ˆï¼‰
                if method == 'pca' or method == 'auto':
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=effective_target_dim, random_state=42)
                    reduced_features = pca.fit_transform(features_array)
                    explained_ratio = np.sum(pca.explained_variance_ratio_)
                    return reduced_features, f"pca_{effective_target_dim}_var{explained_ratio:.3f}_{std_info}"
                else:
                    return features_array[:, :effective_target_dim], f"truncate_{effective_target_dim}_{std_info}"
        
        # å¯¹ä¸‰ä¸ªè§†å›¾åˆ†åˆ«è¿›è¡Œæ™ºèƒ½é™ç»´
        output_reduced, output_method = smart_dimension_reduction(output_grad_features, target_dim=200)
        activation_reduced, activation_method = smart_dimension_reduction(first_activation_features, target_dim=200)
        input_reduced, input_method = smart_dimension_reduction(input_grad_features, target_dim=200)
        
        # è‡ªåŠ¨åŠ æƒï¼šåŸºäºèšç±»åˆ†ç¦»åº¦è®¡ç®—æ¯ä¸ªè§†å›¾çš„è´¡çŒ®æƒé‡
        def compute_view_weights(view_features_list, view_names):
            """åŸºäºæ ‡ç­¾ç¿»è½¬æ£€æµ‹ç†è®ºè®¡ç®—è§†å›¾æƒé‡ - è¾“å‡ºå±‚åº”å ä¸»å¯¼åœ°ä½"""
            from sklearn.metrics import silhouette_score
            from sklearn.cluster import KMeans
            
            weights = []
            scores = []
            
            # å®šä¹‰åŸºäºç†è®ºçš„åŸºç¡€æƒé‡ï¼ˆè¾“å‡ºå±‚æœ€é‡è¦ï¼‰
            theory_based_weights = {
                'Output_Grad': 0.9,      # è¾“å‡ºå±‚æ¢¯åº¦æœ€é‡è¦ - ç›´æ¥åæ˜ æ ‡ç­¾ç¿»è½¬
                'First_Activation': 0.05, # æ¿€æ´»å€¼ç‰¹å¾æ¬¡è¦ - åæ˜ ä¸­é—´è¡¨ç¤ºå˜åŒ–  
                'Input_Grad': 0.05       # è¾“å…¥å±‚æ¢¯åº¦æœ€ä½ - è·ç¦»æ”»å‡»æœ€è¿œ
            }
            
            for i, (view_features, view_name) in enumerate(zip(view_features_list, view_names)):
                # è·å–ç†è®ºæƒé‡ä½œä¸ºåŸºå‡†
                base_weight = theory_based_weights.get(view_name, 0.33)
                
                # å¯¹æ¯ä¸ªè§†å›¾å•ç‹¬åšèšç±»è¯„ä¼°
                kmeans = KMeans(n_clusters=2, random_state=42, n_init=10).fit(view_features)
                labels = kmeans.labels_
                
                # æ£€æŸ¥èšç±»æ˜¯å¦æœ‰æ•ˆï¼ˆä¸¤ä¸ªç°‡éƒ½æœ‰æ ·æœ¬ï¼‰
                if len(set(labels)) > 1:
                    # è®¡ç®—Silhouetteåˆ†æ•°
                    sil_score = silhouette_score(view_features, labels)
                    # è®¡ç®—èšç±»åˆ†ç¦»åº¦ï¼ˆç°‡é—´è·ç¦»/ç°‡å†…è·ç¦»ï¼‰
                    centers = kmeans.cluster_centers_
                    inter_dist = np.linalg.norm(centers[0] - centers[1])
                    intra_dist = np.mean([np.std(view_features[labels == k], axis=0).mean() for k in range(2)])
                    separation_ratio = inter_dist / (intra_dist + 1e-8)
                    
                    # èšç±»è´¨é‡åˆ†æ•° (0-1èŒƒå›´)
                    cluster_quality = max(0, sil_score) + min(1.0, np.log(1 + separation_ratio) * 0.1)
                    cluster_quality = min(1.0, cluster_quality)  # é™åˆ¶åœ¨[0,1]
                else:
                    cluster_quality = 0.0
                    
                # ç»“åˆç†è®ºæƒé‡å’Œèšç±»è´¨é‡ï¼šç†è®ºæƒé‡70%ï¼Œèšç±»è´¨é‡30%
                # è¿™ç¡®ä¿è¾“å‡ºå±‚å§‹ç»ˆä¿æŒè¾ƒé«˜æƒé‡ï¼ŒåŒæ—¶è€ƒè™‘å®é™…æ•°æ®çš„èšç±»æ•ˆæœ
                combined_score = base_weight * 0.7 + cluster_quality * base_weight * 0.3
                scores.append(combined_score)
            
            # æƒé‡å½’ä¸€åŒ–
            scores = np.array(scores)
            weights = scores / np.sum(scores)  # å½’ä¸€åŒ–
            
            return weights, scores
        
        # è®¡ç®—ä¸‰ä¸ªè§†å›¾çš„è‡ªé€‚åº”æƒé‡
        view_features_list = [output_reduced, activation_reduced, input_reduced]
        view_names = ['Output_Grad', 'First_Activation', 'Input_Grad']
        view_weights, view_scores = compute_view_weights(view_features_list, view_names)
        
        # åŠ æƒæ‹¼æ¥é™ç»´åçš„ä¸‰ä¸ªè§†å›¾
        fused_features = []
        for i in range(m):
            # åº”ç”¨è‡ªé€‚åº”æƒé‡
            weighted_output = output_reduced[i] * view_weights[0]
            weighted_activation = activation_reduced[i] * view_weights[1] 
            weighted_input = input_reduced[i] * view_weights[2]
            
            fused = np.concatenate([weighted_output, weighted_activation, weighted_input])
            fused_features.append(fused)
        
        fused_array = np.array(fused_features)
        
        # ä¸€è‡´æ€§çº¦æŸ/å¯¹é½æœºåˆ¶ï¼šèåˆåå†æ•´ä½“æ ‡å‡†åŒ–+å¯é€‰PCA
        def post_fusion_alignment(fused_features, apply_pca=True, final_dim=None):
            """èåˆåä¸€è‡´æ€§å¯¹é½ï¼šæ•´ä½“æ ‡å‡†åŒ– + å¯é€‰PCA"""
            from sklearn.preprocessing import StandardScaler
            
            # æ•´ä½“æ ‡å‡†åŒ–ï¼ˆç¡®ä¿ä¸åŒè§†å›¾èåˆåçš„æ•°å€¼ä¸€è‡´æ€§ï¼‰
            scaler = StandardScaler()
            aligned_features = scaler.fit_transform(fused_features)
            alignment_info = "post_standardized"
            
            # å¯é€‰çš„æ•´ä½“PCAï¼ˆè¿›ä¸€æ­¥é™ç»´å’Œå»ç›¸å…³ï¼‰
            if apply_pca and final_dim is not None:
                n_samples, total_dim = aligned_features.shape
                max_components = min(n_samples, total_dim)
                effective_final_dim = min(final_dim, max_components)
                
                if total_dim > effective_final_dim:
                    from sklearn.decomposition import PCA
                    final_pca = PCA(n_components=effective_final_dim, random_state=42)
                    aligned_features = final_pca.fit_transform(aligned_features)
                    final_explained = np.sum(final_pca.explained_variance_ratio_)
                    alignment_info += f"_pca{effective_final_dim}_var{final_explained:.3f}"
                
            return aligned_features, alignment_info
        
        # åº”ç”¨èåˆåå¯¹é½ï¼ˆå¯é€‰æœ€ç»ˆé™ç»´åˆ°150ç»´ï¼Œä¿æŒåˆç†å¤æ‚åº¦ï¼‰
        final_features, alignment_method = post_fusion_alignment(fused_array, apply_pca=True, final_dim=150)
        
        # ç›´æ¥K-meansèšç±»ï¼ˆæ£€éªŒå¤šæ¨¡æ€èšç±»æ•ˆæœï¼‰
        kmeans = KMeans(n_clusters=2, random_state=0).fit(final_features)
        labels = kmeans.labels_
        
        # ä½¿ç”¨LFDçš„èšç±»è´¨é‡è¯„ä¼°
        clusters = {0: [], 1: []}
        for i, l in enumerate(labels):
            clusters[l].append(final_features[i])
        
        # è®¡ç®—èšç±»è´¨é‡å¹¶ç”Ÿæˆæƒé‡
        cs0, cs1 = lfd.clusters_dissimilarity(clusters)
        good_cl = 0 if cs0 < cs1 else 1
        
        # æ ¹æ®èšç±»è´¨é‡é€‰æ‹©å¥½çš„clusterç”Ÿæˆæƒé‡
        scores = []
        for i in range(m):
            if labels[i] == good_cl:  # é€‰æ‹©è´¨é‡æ›´å¥½çš„cluster
                scores.append(1.0)
            else:  # è´¨é‡è¾ƒå·®çš„cluster
                scores.append(0.0)  # å®Œå…¨æ’é™¤ï¼Œä¸åŸå§‹LFighterä¸€è‡´
        
        # ä½¿ç”¨è®¡ç®—å‡ºçš„scoresè¿›è¡ŒåŠ æƒå¹³å‡ï¼ˆç§»é™¤å¾ªç¯å¯¼å…¥ï¼‰
        aggregated_weights = average_weights(local_weights, scores)
        
        # è¿”å›èšåˆæƒé‡å’Œè§†å›¾æƒé‡ä¿¡æ¯
        view_weights_info = {
            'output_grad': float(view_weights[0]),
            'first_activation': float(view_weights[1]),
            'input_grad': float(view_weights[2])
        }
        
        return aggregated_weights, view_weights_info


class LFighterMVDBO:
    def __init__(self):
        pass
    
    def aggregate(self, simulation_model, local_weights, local_features):
        """LFighter-MV-DBO: é‡ç”¨LFighterMVå¤šè§†å›¾ç‰¹å¾æå– + DBONetè®­ç»ƒ + LFDèšç±»è´¨é‡è¯„ä¼°"""
        import config
        device = simulation_model.device if hasattr(simulation_model, 'device') else config.DEVICE
        
        import torch
        from models import DBONet
        from sklearn.neighbors import kneighbors_graph
        
        # æ£€æŸ¥è¾“å…¥
        if not local_features or len(local_features) == 0:
            raise ValueError("LFighter-MV-DBO requires local_features but got None or empty list.")
        
        m = len(local_weights)
        print(f"[LFighter-MV-DBO] Processing {m} clients with DBONet + LFD quality assessment")
        
        # === æ­¥éª¤1: é‡ç”¨LFighterMVçš„å¤šè§†å›¾ç‰¹å¾æ„å»ºé€»è¾‘ ===
        lfighter_mv = LFighterMV()
        
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„LFDå®ä¾‹ç”¨äºèšç±»è´¨é‡è¯„ä¼°  
        lfd = LFD(config.NUM_CLASSES, attack_ratio=getattr(config, 'ATTACKERS_RATIO', None))
        
        # è°ƒç”¨LFighterMVæ¥è·å–å¤„ç†å¥½çš„å¤šè§†å›¾ç‰¹å¾ï¼ˆä½†ä¸ç”¨å®ƒçš„èšç±»ç»“æœï¼‰
        # æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è°ƒç”¨LFighterMVçš„ç‰¹å¾æå–éƒ¨åˆ†
        local_models = []
        for local_weight in local_weights:
            model = copy.deepcopy(simulation_model)
            model.load_state_dict(local_weight)
            local_models.append(list(model.parameters()))
        
        global_model = list(simulation_model.parameters())
        
        # å¼‚å¸¸ç±»åˆ«æ£€æµ‹ï¼ˆä¸LFighterMVä¸€è‡´ï¼‰
        memory = np.zeros(global_model[-1].shape[0])
        for i in range(m):
            dw = global_model[-2].cpu().data.numpy() - local_models[i][-2].cpu().data.numpy()
            db = global_model[-1].cpu().data.numpy() - local_models[i][-1].cpu().data.numpy()
            norms = np.linalg.norm(dw, axis=-1)
            memory += norms + np.abs(db)
        
        max_two_freq_classes = memory.argsort()[-2:]
        print(f'[LFighter-MV-DBO] Detected anomalous classes: {max_two_freq_classes}')
        
        # è§†å›¾1: è¾“å‡ºå±‚æ¢¯åº¦
        output_grad_features = []
        for i in range(m):
            output_grad = global_model[-2].cpu().data.numpy() - local_models[i][-2].cpu().data.numpy()
            key_classes_grad = output_grad[max_two_freq_classes].reshape(-1)
            output_grad_features.append(key_classes_grad)
        
        # è§†å›¾2: ç¬¬ä¸€å±‚æ¿€æ´»å€¼
        first_activation_features = []
        for i, peer_features in enumerate(local_features):
            actual_features = peer_features
            if isinstance(peer_features, (list, tuple)) and len(peer_features) > 0:
                if isinstance(peer_features[0], (list, tuple)):
                    actual_features = peer_features[0]
            
            first_activation = actual_features[1]
            if not hasattr(first_activation, 'detach'):
                if isinstance(first_activation, (list, tuple)) and len(first_activation) > 0:
                    first_activation = first_activation[0]
            
            first_activation_flat = first_activation[0].detach().cpu().numpy().flatten()
            first_activation_features.append(first_activation_flat)
        
        # è§†å›¾3: è¾“å…¥å±‚æ¢¯åº¦
        input_grad_features = []
        for i in range(m):
            input_grad = global_model[0].cpu().data.numpy() - local_models[i][0].cpu().data.numpy()
            input_grad_features.append(input_grad.reshape(-1))
        
        # === æ­¥éª¤2: ç»Ÿä¸€é™ç»´å¤„ç† ===
        def unified_dimension_reduction(features, target_dim=200, standardize=True):
            features_array = np.array(features)
            n_samples, original_dim = features_array.shape
            
            if standardize:
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                features_array = scaler.fit_transform(features_array)
            
            max_pca_components = min(n_samples, original_dim)
            effective_target_dim = min(target_dim, max_pca_components)
            
            if original_dim > effective_target_dim:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=effective_target_dim, random_state=42)
                features_array = pca.fit_transform(features_array)
                
            return features_array
        
        # åº”ç”¨ç»Ÿä¸€é™ç»´
        output_reduced = unified_dimension_reduction(output_grad_features, target_dim=200)
        activation_reduced = unified_dimension_reduction(first_activation_features, target_dim=200)
        input_reduced = unified_dimension_reduction(input_grad_features, target_dim=200)
        
        # === æ­¥éª¤3: è§†å›¾æƒé‡è®¡ç®—ï¼ˆåŸºäºç†è®ºï¼‰ ===
        view_weights = np.array([0.9, 0.05, 0.05])  # Outputä¸»å¯¼ï¼Œä¸ç†è®ºä¸€è‡´
        print(f"[LFighter-MV-DBO] View weights: Output={view_weights[0]:.3f}, Activation={view_weights[1]:.3f}, Input={view_weights[2]:.3f}")
        
        # === æ­¥éª¤4: DBONetè®­ç»ƒ ===
        # åº”ç”¨æƒé‡å¹¶æ„å»ºå¤šè§†å›¾ç‰¹å¾
        view_features = [
            output_reduced * np.sqrt(view_weights[0]),
            activation_reduced * np.sqrt(view_weights[1]), 
            input_reduced * np.sqrt(view_weights[2])
        ]
        
        nfeats = [f.shape[1] for f in view_features]
        n_view = len(view_features)
        
        # æ„å»ºé‚»æ¥çŸ©é˜µ
        adjs = []
        for v in range(n_view):
            n_neighbors = min(5, m-1)
            adj = kneighbors_graph(view_features[v], n_neighbors=n_neighbors, 
                                 mode='connectivity', include_self=True)
            adj_tensor = torch.tensor(adj.toarray(), dtype=torch.float32, device=device)
            adjs.append(adj_tensor)
        
        # DBONeté…ç½®
        n_clusters = 2
        blocks = 2
        para = 0.05
        np.random.seed(42)
        Z_init = np.random.randn(m, n_clusters) * 0.01
        
        # åˆ›å»ºå’Œè®­ç»ƒDBONet
        dbo_model = DBONet(nfeats, n_view, n_clusters, blocks, para, Z_init, device)
        features_tensor = [torch.tensor(view_features[v], dtype=torch.float32, device=device) 
                         for v in range(n_view)]
        
        # å½’ä¸€åŒ–å¤„ç†
        def normalization(data):
            maxVal = torch.max(data)
            minVal = torch.min(data)
            return (data - minVal) / (maxVal - minVal + 1e-10)

        def standardization(data):
            rowSum = torch.sqrt(torch.sum(data**2, 1))
            repMat = rowSum.repeat((data.shape[1], 1)) + 1e-10
            return torch.div(data, repMat.t())
        
        features_norm = []
        for i in range(n_view):
            feature = features_tensor[i]
            feature = (feature - feature.mean(dim=0)) / (feature.std(dim=0) + 1e-8)
            feature = standardization(normalization(feature))
            features_norm.append(feature)
        
        # è®­ç»ƒDBONet
        dbo_model.train()
        optimizer = torch.optim.Adam(dbo_model.parameters(), lr=5e-4, weight_decay=1e-5)
        criterion = torch.nn.MSELoss()
        
        for epoch in range(8):
            optimizer.zero_grad()
            output_z = dbo_model(features_tensor, adjs)
            
            # åŠ æƒæŸå¤±è®¡ç®—
            loss_dis = torch.tensor(0., device=device)
            loss_lap = torch.tensor(0., device=device)
            
            for k in range(n_view):
                weight = float(view_weights[k])
                target_sim = features_norm[k] @ features_norm[k].t()
                pred_sim = output_z @ output_z.t()
                loss_dis += criterion(pred_sim, target_sim) * weight
                loss_lap += criterion(pred_sim, adjs[k]) * weight
            
            total_loss = loss_dis + 0.3 * loss_lap
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(dbo_model.parameters(), max_norm=1.0)
            optimizer.step()
            
            if epoch % 2 == 0:
                print(f"[LFighter-MV-DBO] Epoch {epoch+1}: Loss={total_loss.item():.6f}")
        
        # === æ­¥éª¤5: ä½¿ç”¨LFDèšç±»è´¨é‡è¯„ä¼°æ›¿æ¢ç®€å•kmeans ===
        dbo_model.eval()
        with torch.no_grad():
            output_z = dbo_model(features_tensor, adjs)
        
        z_np = output_z.detach().cpu().numpy()
        
        # ç®€åŒ–çš„kmeansèšç±»
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10).fit(z_np)
        labels = kmeans.labels_
        
        # === ä½¿ç”¨LFDçš„clusters_dissimilarityè¿›è¡Œè´¨é‡è¯„ä¼° ===
        clusters = {0: [], 1: []}
        for i, l in enumerate(labels):
            clusters[l].append(z_np[i])
        
        # è°ƒç”¨LFDçš„èšç±»è´¨é‡è¯„ä¼°æ–¹æ³•
        cs0, cs1 = lfd.clusters_dissimilarity(clusters)
        good_cl = 0 if cs0 < cs1 else 1
        
        print(f"[LFighter-MV-DBO] LFD cluster quality: cs0={cs0:.4f}, cs1={cs1:.4f}, good_cluster={good_cl}")
        
        # æƒé‡åˆ†é…
        scores = np.ones(m)
        for i, l in enumerate(labels):
            if l != good_cl:
                scores[i] = 0
        
        print(f"[LFighter-MV-DBO] Good clients: {np.sum(scores)}/{m}")
        
        # è¿”å›ç»“æœ
        aggregated_weights = average_weights(local_weights, scores)
        view_weights_info = {
            'output_grad': float(view_weights[0]),
            'first_activation': float(view_weights[1]),
            'input_grad': float(view_weights[2])
        }
        
        return aggregated_weights, view_weights_info


class Tolpegin:
    def __init__(self):
        pass
    
    def score(self, global_model, local_models, peers_types, selected_peers):
        global_model = list(global_model.parameters())
        last_g = global_model[-2].cpu().data.numpy()
        m = len(local_models)
        grads = [None for i in range(m)]
        for i in range(m):
            grad= (last_g - \
                    list(local_models[i].parameters())[-2].cpu().data.numpy())
            grads[i] = grad
        
        grads = np.array(grads)
        num_classes = grad.shape[0]
        # print('Number of classes:', num_classes)
        dist = [ ]
        labels = [ ]
        for c in range(num_classes):
            data = grads[:, c]
            data = get_pca(copy.deepcopy(data))
            kmeans = KMeans(n_clusters=2, random_state=0).fit(data)
            cl = kmeans.cluster_centers_
            dist.append(((cl[0] - cl[1])**2).sum())
            labels.append(kmeans.labels_)
        
        dist = np.array(dist)
        candidate_class = dist.argmax()
        print("Candidate source/target class", candidate_class)
        labels = labels[candidate_class]
        if sum(labels) < m/2:
            scores = 1 - labels
        else:
            scores = labels
        
        for i, pt in enumerate(peers_types):
            print(pt, 'scored', scores[i])
        return scores
#################################################################################################################
# Clip local updates
def clipp_model(g_w, w, gamma =  1):
    for layer in w.keys():
        w[layer] = g_w[layer] + (w[layer] - g_w[layer])*min(1, gamma)
    return w
def FLAME(global_model, local_models, noise_scalar):
    # Compute number of local models
    m = len(local_models)
    
    # Flattent local models
    g_m = np.array([torch.nn.utils.parameters_to_vector(global_model.parameters()).cpu().data.numpy()])
    f_m = np.array([torch.nn.utils.parameters_to_vector(model.parameters()).cpu().data.numpy() for model in local_models])
    grads = g_m - f_m
    # Compute model-wise cosine similarity
    cs = smp.cosine_similarity(grads)
    # Compute the minimum cluster size value
    msc = int(m*0.5) + 1 
    # Apply HDBSCAN on the computed cosine similarities
    clusterer = hdbscan.HDBSCAN(min_cluster_size=msc, min_samples=1, allow_single_cluster = True)
    clusterer.fit(cs)
    labels = clusterer.labels_
    # print('Clusters:', labels)

    if sum(labels) == -(m):
        # In case all of the local models identified as outliers, consider all of as benign
        benign_idxs = np.arange(m)
    else:
        benign_idxs = np.where(labels!=-1)[0]
        
    # Compute euclidean distances to the current global model
    euc_d = cdist(g_m, f_m)[0]
    # Identify the median of computed distances
    st = np.median(euc_d)
    # Clipp admitted updates
    W_c = []
    for i, idx in enumerate(benign_idxs):
        w_c = clipp_model(global_model.state_dict(), local_models[idx].state_dict(), gamma =  st/euc_d[idx])
        W_c.append(w_c)
    
    # Average admitted clipped updates to obtain a new global model
    g_w = average_weights(W_c, np.ones(len(W_c)))
    
    '''From the original paper: {We use standard DP parameters and set eps = 3705 for IC, 
    eps = 395 for the NIDS and eps = 4191 for the NLP scenario. 
    Accordingly, lambda = 0.001 for IC and NLP, and lambda = 0.01 for the NIDS scenario.}
    However, we found lambda = 0.001 with the CIFAR10-ResNet18 benchmark spoils the model
    and therefore we tried lower lambda values, which correspond to greater eps values.'''
    
    # Add adaptive noise to the global model
    lamb = 0.001
    sigma = lamb*st*noise_scalar
    # print('Sigma:{:0.4f}'.format(sigma))
    for key in g_w.keys():
        noise = torch.FloatTensor(g_w[key].shape).normal_(mean=0, std=(sigma**2)).to(g_w[key].device)
        g_w[key] = g_w[key] + noise
        
    return g_w 
#################################################################################################################

def median_opt(input):
    shape = input.shape
    input = input.sort()[0]
    if shape[-1] % 2 != 0:
        output = input[..., int((shape[-1] - 1) / 2)]
    else:
        output = (input[..., int(shape[-1] / 2 - 1)] + input[..., int(shape[-1] / 2)]) / 2.0
    return output

def Repeated_Median_Shard(w):
    SHARD_SIZE = 100000
    w_med = copy.deepcopy(w[0])
    device = w[0][list(w[0].keys())[0]].device

    for k in w_med.keys():
        shape = w_med[k].shape
        if len(shape) == 0:
            continue
        total_num = reduce(lambda x, y: x * y, shape)
        y_list = torch.FloatTensor(len(w), total_num).to(device)
        for i in range(len(w)):
            y_list[i] = torch.reshape(w[i][k], (-1,))
        y = torch.t(y_list)

        if total_num < SHARD_SIZE:
            slopes, intercepts = repeated_median(y)
            y = intercepts + slopes * (len(w) - 1) / 2.0
        else:
            y_result = torch.FloatTensor(total_num).to(device)
            assert total_num == y.shape[0]
            num_shards = int(math.ceil(total_num / SHARD_SIZE))
            for i in range(num_shards):
                y_shard = y[i * SHARD_SIZE: (i + 1) * SHARD_SIZE, ...]
                slopes_shard, intercepts_shard = repeated_median(y_shard)
                y_shard = intercepts_shard + slopes_shard * (len(w) - 1) / 2.0
                y_result[i * SHARD_SIZE: (i + 1) * SHARD_SIZE] = y_shard
            y = y_result
        y = y.reshape(shape)
        w_med[k] = y
    return w_med


def repeated_median(y):
    num_models = y.shape[1]
    total_num = y.shape[0]
    y = y.sort()[0]
    yyj = y.repeat(1, 1, num_models).reshape(total_num, num_models, num_models)
    yyi = yyj.transpose(-1, -2)
    xx = torch.FloatTensor(range(num_models)).to(y.device)
    xxj = xx.repeat(total_num, num_models, 1)
    xxi = xxj.transpose(-1, -2) + eps

    diag = torch.Tensor([float('Inf')] * num_models).to(y.device)
    diag = torch.diag(diag).repeat(total_num, 1, 1)

    dividor = xxi - xxj + diag
    slopes = (yyi - yyj) / dividor + diag
    slopes, _ = slopes.sort()
    slopes = median_opt(slopes[:, :, :-1])
    slopes = median_opt(slopes)

    # get intercepts (intercept of median)
    yy_median = median_opt(y)
    xx_median = [(num_models - 1) / 2.0] * total_num
    xx_median = torch.Tensor(xx_median).to(y.device)
    intercepts = yy_median - slopes * xx_median

    return slopes, intercepts


# Repeated Median estimator
def Repeated_Median(w):
    cur_time = time.time()
    w_med = copy.deepcopy(w[0])
    device = w[0][list(w[0].keys())[0]].device

    for k in w_med.keys():
        shape = w_med[k].shape
        if len(shape) == 0:
            continue
        total_num = reduce(lambda x, y: x * y, shape)
        y_list = torch.FloatTensor(len(w), total_num).to(device)
        for i in range(len(w)):
            y_list[i] = torch.reshape(w[i][k], (-1,))
        y = torch.t(y_list)

        slopes, intercepts = repeated_median(y)
        y = intercepts + slopes * (len(w) - 1) / 2.0

        y = y.reshape(shape)
        w_med[k] = y

    print('repeated median aggregation took {}s'.format(time.time() - cur_time))
    return w_med
        
# simple median estimator
def simple_median(w):
    device = w[0][list(w[0].keys())[0]].device
    w_med = copy.deepcopy(w[0])
    for k in w_med.keys():
        shape = w_med[k].shape
        if len(shape) == 0:
            continue
        total_num = reduce(lambda x, y: x * y, shape)
        y_list = torch.FloatTensor(len(w), total_num).to(device)
        for i in range(len(w)):
            y_list[i] = torch.reshape(w[i][k], (-1,))
        y = torch.t(y_list)
        median_result = median_opt(y)
        assert total_num == len(median_result)

        weight = torch.reshape(median_result, shape)
        w_med[k] = weight
    return w_med

def trimmed_mean(w, trim_ratio):
    if trim_ratio == 0:
        return average_weights(w, [1 for i in range(len(w))])
        
    assert trim_ratio < 0.5, 'trim ratio is {}, but it should be less than 0.5'.format(trim_ratio)
    trim_num = int(trim_ratio * len(w))
    device = w[0][list(w[0].keys())[0]].device
    w_med = copy.deepcopy(w[0])
    for k in w_med.keys():
        shape = w_med[k].shape
        if len(shape) == 0:
            continue
        total_num = reduce(lambda x, y: x * y, shape)
        y_list = torch.FloatTensor(len(w), total_num).to(device)
        for i in range(len(w)):
            y_list[i] = torch.reshape(w[i][k], (-1,))
        y = torch.t(y_list)
        y_sorted = y.sort()[0]
        result = y_sorted[:, trim_num:-trim_num]
        result = result.mean(dim=-1)
        assert total_num == len(result)

        weight = torch.reshape(result, shape)
        w_med[k] = weight
    return w_med


# Get average weights
def average_weights(w, marks):
    """
    Returns the average of the weights.
    """
    w_avg = copy.deepcopy(w[0])
    for key in w_avg.keys():
        w_avg[key] = w_avg[key] * marks[0]
    for key in w_avg.keys():
        for i in range(1, len(w)):
            w_avg[key] += w[i][key] * marks[i]
        w_avg[key] = w_avg[key] *(1/sum(marks))
    return w_avg
   
def Krum(updates, f, multi = False):
    n = len(updates)
    updates = [torch.nn.utils.parameters_to_vector(update.parameters()) for update in updates]
    updates_ = torch.empty([n, len(updates[0])])
    for i in range(n):
      updates_[i] = updates[i]
    k = n - f - 2
    # collection distance, distance from points to points
    cdist = torch.cdist(updates_, updates_, p=2)
    dist, idxs = torch.topk(cdist, k , largest=False)
    dist = dist.sum(1)
    idxs = dist.argsort()
    if multi:
      return idxs[:k]
    else:
      return idxs[0]
##################################################################

class LFighterAutoencoder:
    def __init__(self, num_classes, ae_hidden_dim=128, ae_latent_dim=32, ae_epochs=50, reconstruction_weight=0.8, 
                 enable_visualization=True, save_path="./figures/", 
                 visualization_frequency=1, max_visualizations=0, save_final_only=False,
                 save_as_pdf=True, keep_individual_files=False, attack_ratio=None):
        """
        LFighterAutoencoderç±»åˆå§‹åŒ–
        
        Args:
            num_classes: ç±»åˆ«æ•°é‡
            ae_hidden_dim: è‡ªç¼–ç å™¨éšè—å±‚ç»´åº¦
            ae_latent_dim: è‡ªç¼–ç å™¨æ½œåœ¨ç©ºé—´ç»´åº¦
            ae_epochs: è‡ªç¼–ç å™¨è®­ç»ƒè½®æ•°
            reconstruction_weight: é‡æ„è¯¯å·®æƒé‡
            enable_visualization: æ˜¯å¦å¯ç”¨å¯è§†åŒ–
            save_path: ä¿å­˜è·¯å¾„
            visualization_frequency: å¯è§†åŒ–é¢‘ç‡
            max_visualizations: æœ€å¤§å¯è§†åŒ–æ•°é‡
            save_final_only: æ˜¯å¦åªä¿å­˜æœ€ç»ˆè½®æ¬¡
            save_as_pdf: æ˜¯å¦ä¿å­˜ä¸ºPDF (é»˜è®¤Trueï¼Œå§‹ç»ˆå¯ç”¨)
            keep_individual_files: å§‹ç»ˆç¦ç”¨å•ç‹¬æ–‡ä»¶ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
            attack_ratio: æ”»å‡»è€…æ¯”ç‡ï¼Œç”¨äºPDFæ–‡ä»¶å
            
        ç‰¹æ€§:
            - ä½¿ç”¨è¾“å‡ºå±‚çš„å…¨éƒ¨æ¢¯åº¦ä½œä¸ºç‰¹å¾ï¼Œè€Œä¸æ˜¯ä»…é€‰æ‹©éƒ¨åˆ†ç±»åˆ«
            - é€šè¿‡ä¼ ç»Ÿè‡ªç¼–ç å™¨è¿›è¡Œç‰¹å¾é™ç»´å’Œå¼‚å¸¸æ£€æµ‹
            - ç»“åˆèšç±»å’Œé‡æ„è¯¯å·®è¿›è¡Œæ”»å‡»è€…è¯†åˆ«
            - ä»…ç”ŸæˆPDFæŠ¥å‘Šï¼Œä¸ç”Ÿæˆå•ç‹¬æ–‡ä»¶
        """
        self.num_classes = num_classes
        self.memory = np.zeros(num_classes)
        
        # Autoencoderç›¸å…³å‚æ•°
        self.ae_hidden_dim = ae_hidden_dim
        self.ae_latent_dim = ae_latent_dim
        self.ae_epochs = ae_epochs
        self.reconstruction_weight = reconstruction_weight
    
        # å¯è§†åŒ–ç›¸å…³å‚æ•°
        self.enable_visualization = enable_visualization
        self.save_path = save_path
        self.visualization_frequency = visualization_frequency
        self.max_visualizations = max_visualizations
        self.save_final_only = save_final_only
        self.save_as_pdf = True  # å§‹ç»ˆä¿å­˜ä¸ºPDFï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        self.keep_individual_files = False  # å§‹ç»ˆç¦ç”¨å•ç‹¬æ–‡ä»¶ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        self.attack_ratio = attack_ratio
        
        # è½®æ•°è®¡æ•°å™¨
        self.round_counter = 0
        self.total_rounds = None
        self.visualization_count = 0
        
        # PDFæ–‡ä»¶ç®¡ç†
        self.pdf_pages = None
        self.pdf_filename = None
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        if self.enable_visualization:
            os.makedirs(self.save_path, exist_ok=True)
            
            # åˆ›å»ºPDFæ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨PDFä¿å­˜ï¼‰
            if self.save_as_pdf:
                self._initialize_pdf()
    
    def _initialize_pdf(self):
        """åˆå§‹åŒ–PDFæ–‡ä»¶"""
        try:
            from matplotlib.backends.backend_pdf import PdfPages
            attack_str = f"_atr{self.attack_ratio:.1f}" if self.attack_ratio is not None else ""
            self.pdf_filename = f'{self.save_path}/lfighter_ae_complete_report{attack_str}.pdf'
            self.pdf_pages = PdfPages(self.pdf_filename)
            print(f"[LFighter-AE] ğŸ”— PDFæŠ¥å‘Šåˆå§‹åŒ–æˆåŠŸ")
            print(f"[LFighter-AE] ğŸ“„ å®æ—¶æŸ¥çœ‹è·¯å¾„: {self.pdf_filename}")
        except Exception as e:
            print(f"[LFighter-AE] PDFåˆå§‹åŒ–å¤±è´¥: {e}")
            self.pdf_pages = None
    
    def finalize_pdf(self):
        """å…³é—­PDFæ–‡ä»¶"""
        if self.pdf_pages is not None:
            try:
                self.pdf_pages.close()
                print(f"[LFighter-AE] PDFæŠ¥å‘Šå·²ä¿å­˜: {self.pdf_filename}")
            except Exception as e:
                print(f"[LFighter-AE] PDFå…³é—­å¤±è´¥: {e}")
            finally:
                self.pdf_pages = None
    
    def set_total_rounds(self, total_rounds):
        """è®¾ç½®æ€»è®­ç»ƒè½®æ•°ï¼Œç”¨äºsave_final_onlyæ¨¡å¼"""
        self.total_rounds = total_rounds
        print(f"[LFighter-AE] Set total rounds to {total_rounds} for visualization control")
    
    def should_visualize_this_round(self):
        """åˆ¤æ–­å½“å‰è½®æ¬¡æ˜¯å¦åº”è¯¥ä¿å­˜å¯è§†åŒ–"""
        if not self.enable_visualization:
            return False
            
        # å¦‚æœè®¾ç½®ä¸ºåªä¿å­˜æœ€åä¸€è½®
        if self.save_final_only:
            return self.total_rounds is not None and self.round_counter == self.total_rounds
        
        # åŠ¨æ€è°ƒæ•´å¯è§†åŒ–é¢‘ç‡ï¼šå‰20è½®æ¯è½®å¯è§†åŒ–ï¼Œåç»­æ¯10è½®ä¸€æ¬¡
        if self.round_counter <= 20:
            current_frequency = 1
        else:
            current_frequency = 10
        
        # æŒ‰åŠ¨æ€é¢‘ç‡ä¿å­˜
        return self.round_counter % current_frequency == 0
    
    def cleanup_old_visualizations(self):
        """æ¸…ç†æ—§çš„å¯è§†åŒ–æ–‡ä»¶"""
        if not self.enable_visualization:
            return
        
        if self.max_visualizations > 0 and self.visualization_count > self.max_visualizations:
            # æ¸…ç†PNGæ–‡ä»¶
            png_files = [f for f in os.listdir(self.save_path) if f.startswith('lfighter_') and f.endswith('.png')]
            png_files.sort(key=lambda x: os.path.getctime(os.path.join(self.save_path, x)))
            
            while len(png_files) > self.max_visualizations:
                old_file = os.path.join(self.save_path, png_files.pop(0))
                if os.path.exists(old_file):
                    os.remove(old_file)
            
            # æ¸…ç†æ–‡æœ¬æ–‡ä»¶
            txt_files = [f for f in os.listdir(self.save_path) if f.startswith('lfighter_') and f.endswith('.txt')]
            txt_files.sort(key=lambda x: os.path.getctime(os.path.join(self.save_path, x)))
            
            while len(txt_files) > self.max_visualizations:
                old_file = os.path.join(self.save_path, txt_files.pop(0))
                if os.path.exists(old_file):
                    os.remove(old_file)
    
    def create_pdf_report(self, round_num, features, labels, ptypes, scores, cs0, cs1, good_cl, metrics):
        """å‘å·²æœ‰PDFæ–‡ä»¶æ·»åŠ å½“å‰è½®æ¬¡çš„å¯è§†åŒ–é¡µé¢ - LFighter-AEç‰ˆæœ¬"""
        if not self.should_visualize_this_round() or not self.save_as_pdf or self.pdf_pages is None:
            return
        
        try:
            # ç¬¬ä¸€é¡µï¼šç‰¹å¾ç©ºé—´å¯è§†åŒ–ï¼ˆåŸå§‹vsæ½œåœ¨ç‰¹å¾ï¼‰
            fig, axes = plt.subplots(1, 2, figsize=(16, 8))
            
            # t-SNEé™ç»´
            if features.shape[1] > 2:
                try:
                    perplexity = min(30, len(features)-1)
                    if perplexity < 1:
                        perplexity = 1
                    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
                    features_2d = tsne.fit_transform(features)
                except:
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=2, random_state=42)
                    features_2d = pca.fit_transform(features)
            else:
                features_2d = features
            
            # æŒ‰å®¢æˆ·ç«¯ç±»å‹ç€è‰²
            colors = []
            malicious_count = 0
            for ptype in ptypes:
                ptype_str = str(ptype).lower()
                if ('malicious' in ptype_str or 'attack' in ptype_str or 
                    'bad' in ptype_str or 'adversarial' in ptype_str):
                    colors.append('red')
                    malicious_count += 1
                else:
                    colors.append('blue')
            
            axes[0].scatter(features_2d[:, 0], features_2d[:, 1], c=colors, alpha=0.7, s=100)
            axes[0].set_title('LFighter-AE: Feature Space (by Client Type)', fontsize=14)
            axes[0].set_xlabel('Dimension 1')
            axes[0].set_ylabel('Dimension 2')
            
            red_patch = plt.matplotlib.patches.Patch(color='red', label='Malicious')
            blue_patch = plt.matplotlib.patches.Patch(color='blue', label='Benign')
            axes[0].legend(handles=[red_patch, blue_patch])
            
            # æŒ‰èšç±»ç»“æœç€è‰²
            cluster_colors = ['orange', 'green']
            for i in range(len(features_2d)):
                axes[1].scatter(features_2d[i, 0], features_2d[i, 1], 
                               c=cluster_colors[labels[i]], alpha=0.7, s=100)
            axes[1].set_title('LFighter-AE: Feature Space (by Cluster)', fontsize=14)
            axes[1].set_xlabel('Dimension 1')
            axes[1].set_ylabel('Dimension 2')
            
            orange_patch = plt.matplotlib.patches.Patch(color='orange', label='Cluster 0')
            green_patch = plt.matplotlib.patches.Patch(color='green', label='Cluster 1')
            axes[1].legend(handles=[orange_patch, green_patch])
            
            plt.suptitle(f'LFighter-AE Algorithm - Round {round_num} - Feature Space Analysis', fontsize=16)
            plt.tight_layout()
            self.pdf_pages.savefig(fig, bbox_inches='tight')
            plt.close()
            
            # ç¬¬äºŒé¡µï¼šèšç±»è´¨é‡å’Œå®¢æˆ·ç«¯å¾—åˆ†
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            
            # èšç±»è´¨é‡
            clusters = ['Cluster 0', 'Cluster 1']
            dissimilarities = [cs0, cs1]
            cluster_quality_colors = ['green' if i == good_cl else 'red' for i in range(2)]
            
            bars = axes[0, 0].bar(clusters, dissimilarities, color=cluster_quality_colors, alpha=0.7)
            axes[0, 0].set_ylabel('Dissimilarity Score')
            axes[0, 0].set_title('Cluster Quality Comparison')
            axes[0, 0].grid(True, alpha=0.3)
            
            for i, (bar, score) in enumerate(zip(bars, dissimilarities)):
                axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                              f'{score:.4f}', ha='center', va='bottom')
            
            axes[0, 0].text(good_cl, dissimilarities[good_cl] + 0.05, 'âœ“ Good Cluster', 
                           ha='center', va='bottom', fontsize=12, fontweight='bold')
            
            # å®¢æˆ·ç«¯å¾—åˆ†æ¡å½¢å›¾
            client_indices = range(len(scores))
            bars = axes[0, 1].bar(client_indices, scores, color=colors, alpha=0.7)
            axes[0, 1].set_title('Client Scores')
            axes[0, 1].set_xlabel('Client Index')
            axes[0, 1].set_ylabel('Score')
            axes[0, 1].set_ylim(0, 1.1)
            axes[0, 1].grid(True, alpha=0.3)
            
            # å¾—åˆ†åˆ†å¸ƒç›´æ–¹å›¾
            axes[1, 0].hist(scores, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
            axes[1, 0].axvline(np.mean(scores), color='red', linestyle='--', 
                              label=f'Mean: {np.mean(scores):.4f}')
            axes[1, 0].axvline(np.median(scores), color='orange', linestyle='--', 
                              label=f'Median: {np.median(scores):.4f}')
            axes[1, 0].set_xlabel('Score')
            axes[1, 0].set_ylabel('Frequency')
            axes[1, 0].set_title('Score Distribution')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
            
            # æ€§èƒ½æŒ‡æ ‡æ–‡æœ¬
            axes[1, 1].axis('off')
            attack_ratio_str = f"Attack Ratio: {self.attack_ratio:.1f}" if self.attack_ratio is not None else "Attack Ratio: N/A"
            metrics_text = f"""
Performance Metrics - Round {round_num}

{attack_ratio_str}
Total Clients: {metrics.get('total_clients', 'N/A')}
Good Clients Selected: {metrics.get('good_clients', 'N/A')}
Selection Accuracy: {metrics.get('accuracy', 'N/A'):.2%}

KMeans Clustering:
â€¢ Method: {metrics.get('clustering_method', 'N/A')}
â€¢ Number of Clusters: {metrics.get('n_clusters', 'N/A')}
â€¢ Cluster Labels: {metrics.get('cluster_labels', 'N/A')}
â€¢ Cluster Distribution: {metrics.get('cluster_counts', 'N/A')}

Cluster Analysis:
â€¢ Cluster 0 Dissimilarity: {metrics.get('cs0', 'N/A'):.4f}
â€¢ Cluster 1 Dissimilarity: {metrics.get('cs1', 'N/A'):.4f}
â€¢ Good Cluster: {metrics.get('good_cluster', 'N/A')}

Feature Processing:
â€¢ Reduction Method: {metrics.get('reduction_method', 'N/A')}
â€¢ Attack Scope: {metrics.get('attack_scope', 'N/A')}
â€¢ Attack Type: {metrics.get('attack_type', 'N/A')}

AE (Autoencoder):
â€¢ Reconstruction Weight: {metrics.get('fixed_recon_weight', 'N/A'):.2f}
â€¢ Mean Recon Error: {metrics.get('mean_recon_error', 'N/A'):.4f}

Algorithm: LFighter + AE + KMeans
Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}
"""
            axes[1, 1].text(0.05, 0.95, metrics_text, transform=axes[1, 1].transAxes, 
                           fontsize=10, verticalalignment='top', fontfamily='monospace')
            
            plt.suptitle(f'LFighter-AE Algorithm - Round {round_num} - Performance Analysis', fontsize=16)
            plt.tight_layout()
            self.pdf_pages.savefig(fig, bbox_inches='tight')
            plt.close()
            
            # å¼ºåˆ¶åˆ·æ–°PDFæ–‡ä»¶ï¼Œç¡®ä¿å®æ—¶å¯è§
            try:
                # matplotlib PdfPages çš„æ­£ç¡®flushæ–¹æ³•
                if hasattr(self.pdf_pages, '_file') and hasattr(self.pdf_pages._file, '_file'):
                    self.pdf_pages._file._file.flush()
                    import os
                    os.fsync(self.pdf_pages._file._file.fileno())
                elif hasattr(self.pdf_pages, 'flush'):
                    self.pdf_pages.flush()
            except:
                pass  # å¿½ç•¥flushé”™è¯¯ï¼Œä¸å½±å“æ­£å¸¸åŠŸèƒ½
            
            print(f"[LFighter-AE] Round {round_num} æ·»åŠ åˆ°PDFæŠ¥å‘Š - å®æ—¶å¯æŸ¥çœ‹: {self.pdf_filename}")
            
        except Exception as e:
            print(f"[LFighter-AE] PDFé¡µé¢æ·»åŠ å¤±è´¥ Round {round_num}: {e}")
    
    def visualize_training_process(self, losses, round_num):
        """å¯è§†åŒ–autoencoderè®­ç»ƒè¿‡ç¨‹"""
        if not self.should_visualize_this_round():
            return
            
        # åªåœ¨PDFæŠ¥å‘Šä¸­æ·»åŠ è®­ç»ƒè¿‡ç¨‹å›¾è¡¨ï¼Œä¸ç”Ÿæˆå•ç‹¬çš„å›¾ç‰‡æ–‡ä»¶
        if self.save_as_pdf and self.pdf_pages is not None:
            plt.figure(figsize=(10, 6))
            plt.plot(losses, 'b-', linewidth=2, label='Reconstruction Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.title(f'AE Training Process (Round {round_num})')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            try:
                self.pdf_pages.savefig(plt.gcf(), bbox_inches='tight')
            except Exception as e:
                print(f"[LFighter-AE] è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–æ·»åŠ åˆ°PDFå¤±è´¥: {e}")
            plt.close()
    
    def visualize_feature_space(self, original_features, latent_features, labels, ptypes, round_num):
        """å¯è§†åŒ–åŸå§‹å’Œæ½œåœ¨ç‰¹å¾ç©ºé—´çš„å¯¹æ¯”"""
        if not self.should_visualize_this_round():
            return
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å®¢æˆ·ç«¯ç±»å‹
        if self.enable_visualization and round_num == 1:  # åªåœ¨ç¬¬ä¸€è½®æ‰“å°
            print(f"[LFighter-AE Debug] Client types: {ptypes[:10]}...")  # æ‰“å°å‰10ä¸ªå®¢æˆ·ç«¯ç±»å‹
        
        # åªåœ¨PDFæŠ¥å‘Šä¸­æ·»åŠ ç‰¹å¾ç©ºé—´å¯è§†åŒ–ï¼Œä¸ç”Ÿæˆå•ç‹¬çš„å›¾ç‰‡æ–‡ä»¶
        if self.save_as_pdf and self.pdf_pages is not None:
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            
            # å¯¹ä¸¤ç§ç‰¹å¾ç©ºé—´è¿›è¡Œt-SNEé™ç»´
            features_list = [original_features, latent_features]
            feature_names = ['Original Features', 'Latent Features']
            
            for row, (features, name) in enumerate(zip(features_list, feature_names)):
                # t-SNEé™ç»´
                if features.shape[1] > 2:
                    try:
                        perplexity = min(30, len(features)-1)
                        if perplexity < 1:
                            perplexity = 1
                        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
                        features_2d = tsne.fit_transform(features)
                    except:
                        # å¦‚æœt-SNEå¤±è´¥ï¼Œä½¿ç”¨PCA
                        from sklearn.decomposition import PCA
                        pca = PCA(n_components=2, random_state=42)
                        features_2d = pca.fit_transform(features)
                else:
                    features_2d = features
                
                # 1. æŒ‰å®¢æˆ·ç«¯ç±»å‹ç€è‰² - å¢å¼ºæ£€æµ‹é€»è¾‘
                colors = []
                malicious_count = 0
                for ptype in ptypes:
                    ptype_str = str(ptype).lower()
                    if ('malicious' in ptype_str or 'attack' in ptype_str or 
                        'bad' in ptype_str or 'adversarial' in ptype_str):
                        colors.append('red')
                        malicious_count += 1
                    else:
                        colors.append('blue')
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ£€æµ‹ç»“æœï¼ˆåªåœ¨ç¬¬ä¸€è½®å’ŒåŸå§‹ç‰¹å¾æ—¶æ‰“å°ï¼‰
                if self.enable_visualization and round_num == 1 and row == 0:
                    print(f"[LFighter-AE Debug] Detected {malicious_count}/{len(ptypes)} malicious clients")
                
                axes[row, 0].scatter(features_2d[:, 0], features_2d[:, 1], c=colors, alpha=0.7, s=100)
                axes[row, 0].set_title(f'{name} (by Client Type)')
                axes[row, 0].set_xlabel('Dimension 1')
                axes[row, 0].set_ylabel('Dimension 2')
                
                # æ·»åŠ å›¾ä¾‹
                red_patch = plt.matplotlib.patches.Patch(color='red', label='Malicious')
                blue_patch = plt.matplotlib.patches.Patch(color='blue', label='Benign')
                axes[row, 0].legend(handles=[red_patch, blue_patch])
                
                # 2. æŒ‰èšç±»ç»“æœç€è‰²
                cluster_colors = ['orange', 'green']
                for i in range(len(features_2d)):
                    axes[row, 1].scatter(features_2d[i, 0], features_2d[i, 1], 
                                       c=cluster_colors[labels[i]], alpha=0.7, s=100)
                axes[row, 1].set_title(f'{name} (by Cluster)')
                axes[row, 1].set_xlabel('Dimension 1')
                axes[row, 1].set_ylabel('Dimension 2')
                
                # æ·»åŠ èšç±»å›¾ä¾‹
                orange_patch = plt.matplotlib.patches.Patch(color='orange', label='Cluster 0')
                green_patch = plt.matplotlib.patches.Patch(color='green', label='Cluster 1')
                axes[row, 1].legend(handles=[orange_patch, green_patch])
            
            plt.tight_layout()
            try:
                self.pdf_pages.savefig(fig, bbox_inches='tight')
            except Exception as e:
                print(f"[LFighter-AE] ç‰¹å¾ç©ºé—´å¯è§†åŒ–æ·»åŠ åˆ°PDFå¤±è´¥: {e}")
            plt.close()
    
    def visualize_reconstruction_errors(self, reconstruction_errors, ptypes, round_num):
        """å¯è§†åŒ–é‡æ„è¯¯å·®åˆ†å¸ƒå’Œå„å®¢æˆ·ç«¯çš„é‡æ„è¯¯å·®"""
        if not self.should_visualize_this_round():
            return
        
        # åªåœ¨PDFæŠ¥å‘Šä¸­æ·»åŠ é‡æ„è¯¯å·®å¯è§†åŒ–ï¼Œä¸ç”Ÿæˆå•ç‹¬çš„å›¾ç‰‡æ–‡ä»¶
        if self.save_as_pdf and self.pdf_pages is not None:
            fig, axes = plt.subplots(1, 2, figsize=(16, 6))
            
            # 1. é‡æ„è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
            axes[0].hist(reconstruction_errors, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
            axes[0].axvline(np.mean(reconstruction_errors), color='red', linestyle='--', 
                           label=f'Mean: {np.mean(reconstruction_errors):.4f}')
            axes[0].axvline(np.median(reconstruction_errors), color='orange', linestyle='--', 
                           label=f'Median: {np.median(reconstruction_errors):.4f}')
            axes[0].set_xlabel('Reconstruction Error')
            axes[0].set_ylabel('Frequency')
            axes[0].set_title('Reconstruction Error Distribution')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)
            
            # 2. å„å®¢æˆ·ç«¯é‡æ„è¯¯å·®æ¡å½¢å›¾ - å¢å¼ºæ£€æµ‹é€»è¾‘
            client_indices = range(len(reconstruction_errors))
            colors = []
            for ptype in ptypes:
                ptype_str = str(ptype).lower()
                if ('malicious' in ptype_str or 'attack' in ptype_str or 
                    'bad' in ptype_str or 'adversarial' in ptype_str):
                    colors.append('red')
                else:
                    colors.append('blue')
            
            bars = axes[1].bar(client_indices, reconstruction_errors, color=colors, alpha=0.7)
            axes[1].set_title('Client Reconstruction Errors')
            axes[1].set_xlabel('Client Index')
            axes[1].set_ylabel('Reconstruction Error')
            axes[1].grid(True, alpha=0.3)
            
            # æ·»åŠ å›¾ä¾‹
            red_patch = plt.matplotlib.patches.Patch(color='red', label='Malicious')
            blue_patch = plt.matplotlib.patches.Patch(color='blue', label='Benign')
            axes[1].legend(handles=[red_patch, blue_patch])
            
            plt.tight_layout()
            try:
                self.pdf_pages.savefig(fig, bbox_inches='tight')
            except Exception as e:
                print(f"[LFighter-AE] é‡æ„è¯¯å·®å¯è§†åŒ–æ·»åŠ åˆ°PDFå¤±è´¥: {e}")
            plt.close()
    
    def visualize_client_scores(self, cluster_scores, recon_scores, final_scores, ptypes, round_num):
        """å¯è§†åŒ–å®¢æˆ·ç«¯çš„èšç±»å¾—åˆ†ã€é‡æ„å¾—åˆ†å’Œæœ€ç»ˆå¾—åˆ†"""
        if not self.should_visualize_this_round():
            return
        
        # åªåœ¨PDFæŠ¥å‘Šä¸­æ·»åŠ å®¢æˆ·ç«¯å¾—åˆ†å¯è§†åŒ–ï¼Œä¸ç”Ÿæˆå•ç‹¬çš„å›¾ç‰‡æ–‡ä»¶
        if self.save_as_pdf and self.pdf_pages is not None:
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            
            client_indices = range(len(final_scores))
            
            # å¢å¼ºçš„å®¢æˆ·ç«¯ç±»å‹æ£€æµ‹é€»è¾‘
            colors = []
            malicious_count = 0
            for ptype in ptypes:
                ptype_str = str(ptype).lower()
                if ('malicious' in ptype_str or 'attack' in ptype_str or 
                    'bad' in ptype_str or 'adversarial' in ptype_str):
                    colors.append('red')
                    malicious_count += 1
                else:
                    colors.append('blue')
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ£€æµ‹ç»“æœ
            if self.enable_visualization and round_num == 1:
                print(f"[LFighter-AE Client Scores] Detected {malicious_count}/{len(ptypes)} malicious clients")
            
            # 1. èšç±»å¾—åˆ†
            axes[0, 0].bar(client_indices, cluster_scores, color=colors, alpha=0.7)
            axes[0, 0].set_title('Cluster Scores')
            axes[0, 0].set_xlabel('Client Index')
            axes[0, 0].set_ylabel('Cluster Score')
            axes[0, 0].set_ylim(0, 1.1)
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. é‡æ„å¾—åˆ†
            axes[0, 1].bar(client_indices, recon_scores, color=colors, alpha=0.7)
            axes[0, 1].set_title('Reconstruction Scores')
            axes[0, 1].set_xlabel('Client Index')
            axes[0, 1].set_ylabel('Reconstruction Score')
            axes[0, 1].set_ylim(0, 1.1)
            axes[0, 1].grid(True, alpha=0.3)
            
            # 3. æœ€ç»ˆå¾—åˆ†
            axes[1, 0].bar(client_indices, final_scores, color=colors, alpha=0.7)
            axes[1, 0].set_title('Final Scores (Weighted Combination)')
            axes[1, 0].set_xlabel('Client Index')
            axes[1, 0].set_ylabel('Final Score')
            axes[1, 0].set_ylim(0, 1.1)
            axes[1, 0].grid(True, alpha=0.3)
            
            # æ·»åŠ å›¾ä¾‹
            red_patch = plt.matplotlib.patches.Patch(color='red', label='Malicious')
            blue_patch = plt.matplotlib.patches.Patch(color='blue', label='Benign')
            axes[1, 0].legend(handles=[red_patch, blue_patch])
            
            # 4. å¾—åˆ†å¯¹æ¯”æ•£ç‚¹å›¾
            axes[1, 1].scatter(cluster_scores, recon_scores, c=colors, alpha=0.7, s=100)
            axes[1, 1].set_xlabel('Cluster Score')
            axes[1, 1].set_ylabel('Reconstruction Score')
            axes[1, 1].set_title('Cluster vs Reconstruction Scores')
            axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            try:
                self.pdf_pages.savefig(fig, bbox_inches='tight')
            except Exception as e:
                print(f"[LFighter-AE] å®¢æˆ·ç«¯å¾—åˆ†å¯è§†åŒ–æ·»åŠ åˆ°PDFå¤±è´¥: {e}")
            plt.close()
    
    
    def visualize_cluster_quality(self, cs0, cs1, good_cl, round_num):
        """å¯è§†åŒ–èšç±»è´¨é‡"""
        if not self.should_visualize_this_round():
            return
            
        # åªåœ¨PDFæŠ¥å‘Šä¸­æ·»åŠ èšç±»è´¨é‡å¯è§†åŒ–ï¼Œä¸ç”Ÿæˆå•ç‹¬çš„å›¾ç‰‡æ–‡ä»¶
        if self.save_as_pdf and self.pdf_pages is not None:
            plt.figure(figsize=(10, 6))
            clusters = ['Cluster 0', 'Cluster 1']
            dissimilarities = [cs0, cs1]
            colors = ['green' if i == good_cl else 'red' for i in range(2)]
            
            plt.bar(clusters, dissimilarities, color=colors, alpha=0.7)
            plt.ylabel('Dissimilarity Score')
            plt.title('Cluster Quality Comparison')
            plt.grid(True, alpha=0.3)
            
            # æ·»åŠ å›¾ä¾‹
            green_patch = plt.matplotlib.patches.Patch(color='green', label='Good Cluster')
            red_patch = plt.matplotlib.patches.Patch(color='red', label='Bad Cluster')
            plt.legend(handles=[green_patch, red_patch])
            
            try:
                self.pdf_pages.savefig(plt.gcf(), bbox_inches='tight')
            except Exception as e:
                print(f"[LFighter-AE] èšç±»è´¨é‡å¯è§†åŒ–æ·»åŠ åˆ°PDFå¤±è´¥: {e}")
            plt.close()
    
            plt.close()
    
            if attack_scope == 'large_multi':
                analysis['attack_type'] = 'large_multi_target'
                analysis['confidence'] = 0.85
                analysis['pattern_description'] = f'å¤§è§„æ¨¡å¤šæ ‡ç­¾æ”»å‡»ï¼Œæ¶‰åŠ{len(selected_classes)}ä¸ªç±»åˆ«'
            elif attack_scope == 'small_multi':
                high_score_classes = selected_classes[relative_scores > 0.7]
                if len(high_score_classes) >= 3:
                    analysis['attack_type'] = 'small_multi_target'
                    analysis['confidence'] = 0.8
                    analysis['pattern_description'] = f'å°è§„æ¨¡å¤šæ ‡ç­¾æ”»å‡»ï¼Œ{len(high_score_classes)}ä¸ªé«˜å½±å“ç±»åˆ«'
                else:
                    analysis['attack_type'] = 'complex_sparse'
                    analysis['confidence'] = 0.6
                    analysis['pattern_description'] = f'å¤æ‚ç¨€ç–æ”»å‡»æ¨¡å¼ï¼Œæ¶‰åŠ{len(selected_classes)}ä¸ªç±»åˆ«'
            elif attack_scope == 'traditional':
                analysis['attack_type'] = 'simple_targeted'
                analysis['confidence'] = 0.9
                analysis['pattern_description'] = 'ä¼ ç»Ÿå•æº-å•ç›®æ ‡æ”»å‡»'
            else:
                analysis['attack_type'] = 'unknown_local'
                analysis['confidence'] = 0.3
                analysis['pattern_description'] = 'æœªçŸ¥å±€éƒ¨æ”»å‡»æ¨¡å¼'
        
        print(f'[LFighter-AE] æ”»å‡»æ¨¡å¼åˆ†æ: {analysis["attack_type"]} (confidence: {analysis["confidence"]:.2f})')
        print(f'[LFighter-AE] æ¨¡å¼æè¿°: {analysis["pattern_description"]}')
        print(f'[LFighter-AE] æ”»å‡»èŒƒå›´: {attack_scope}')
        
        return analysis
        
    def clusters_dissimilarity(self, clusters):
        """è®¡ç®—èšç±»é—´ç›¸å¼‚æ€§ï¼Œå¤„ç†ç©ºèšç±»æƒ…å†µ"""
        n0 = len(clusters[0])
        n1 = len(clusters[1])
        m = n0 + n1 
        
        # å¤„ç†ç©ºèšç±»æƒ…å†µ
        if n0 == 0:
            return 1.0, 0.0  # ç©ºèšç±»0ï¼Œèšç±»1æ›´å¥½
        if n1 == 0:
            return 0.0, 1.0  # ç©ºèšç±»1ï¼Œèšç±»0æ›´å¥½
        if n0 == 1:
            ds0 = 1.0  # å•æ ·æœ¬èšç±»è´¨é‡è®¾ä¸ºæœ€å·®
        else:
            cs0 = smp.cosine_similarity(clusters[0]) - np.eye(n0)
            mincs0 = np.min(cs0, axis=1)
            ds0 = n0/m * (1 - np.mean(mincs0))
        
        if n1 == 1:
            ds1 = 1.0  # å•æ ·æœ¬èšç±»è´¨é‡è®¾ä¸ºæœ€å·®
        else:
            cs1 = smp.cosine_similarity(clusters[1]) - np.eye(n1)
            mincs1 = np.min(cs1, axis=1)
            ds1 = n1/m * (1 - np.mean(mincs1))
        
        return ds0, ds1
    
    def create_autoencoder(self, input_dim, device):
        """åˆ›å»ºAEæ¨¡å‹"""
        class AE(nn.Module):
            def __init__(self, input_dim, hidden_dim, latent_dim):
                super(AE, self).__init__()
                # ç¼–ç å™¨
                self.encoder = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.BatchNorm1d(hidden_dim),
                    nn.Dropout(0.1),
                    nn.Linear(hidden_dim, latent_dim)
                )
                
                # è§£ç å™¨
                self.decoder = nn.Sequential(
                    nn.Linear(latent_dim, hidden_dim),
                    nn.ReLU(),
                    nn.BatchNorm1d(hidden_dim),
                    nn.Dropout(0.1),
                    nn.Linear(hidden_dim, input_dim)
                )
            
            def encode(self, x):
                return self.encoder(x)
            
            def decode(self, z):
                return self.decoder(z)
            
            def forward(self, x):
                z = self.encode(x)
                decoded = self.decode(z)
                return z, decoded
        
        return AE(input_dim, self.ae_hidden_dim, self.ae_latent_dim).to(device)
    
    def train_autoencoder(self, features, device):
        """è®­ç»ƒAEå¹¶è¿”å›æ½œåœ¨è¡¨ç¤ºå’Œé‡æ„è¯¯å·®"""
        features_tensor = torch.FloatTensor(features).to(device)
        input_dim = features_tensor.shape[1]
        
        # åˆ›å»ºAE
        ae = self.create_autoencoder(input_dim, device)
        optimizer = optim.Adam(ae.parameters(), lr=0.001, weight_decay=1e-5)
        
        # AEæŸå¤±å‡½æ•° - åªæœ‰é‡æ„æŸå¤±
        criterion = nn.MSELoss(reduction='mean')
        
        # è®°å½•è®­ç»ƒæŸå¤±ç”¨äºå¯è§†åŒ–
        training_losses = []
        
        # è®­ç»ƒAE
        ae.train()
        for epoch in range(self.ae_epochs):
            optimizer.zero_grad()
            z, decoded = ae(features_tensor)
            
            # è®¡ç®—é‡æ„æŸå¤±
            recon_loss = criterion(decoded, features_tensor)
            recon_loss.backward()
            optimizer.step()
            
            # è®°å½•æŸå¤±
            training_losses.append(recon_loss.item())
            
            if epoch % 10 == 0:
                print(f'[LFighter-AE] AE Epoch {epoch+1}/{self.ae_epochs}: Recon Loss={recon_loss.item():.6f}')
        
        # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
        if self.should_visualize_this_round():
            self.visualize_training_process(training_losses, self.round_counter)
        
        # è·å–æ½œåœ¨è¡¨ç¤ºå’Œé‡æ„è¯¯å·®
        ae.eval()
        with torch.no_grad():
            z, decoded = ae(features_tensor)
            reconstruction_errors = torch.mean((features_tensor - decoded) ** 2, dim=1)
        
        return z.cpu().numpy(), reconstruction_errors.cpu().numpy(), training_losses[-1]
    
    def aggregate(self, global_model, local_models, ptypes):
        """åŸºäºLFighter + ä¼ ç»ŸAutoencoderçš„èšåˆæ–¹æ³•"""
        import config
        device = getattr(config, 'DEVICE', 'cpu')
        
        # å¢åŠ è½®æ•°è®¡æ•°
        self.round_counter += 1
        # æç¤ºå½“å‰å¯è§†åŒ–é¢‘ç‡
        current_freq = 1 if self.round_counter <= 20 else 10
        next_viz_round = self.round_counter if self.round_counter % current_freq == 0 else ((self.round_counter // current_freq) + 1) * current_freq
        print(f"[LFighter-AE] Round {self.round_counter} - Visualization freq: every {current_freq} rounds (next: round {next_viz_round})")
        
        local_weights = [copy.deepcopy(model).state_dict() for model in local_models]
        m = len(local_models)
        
        # è½¬æ¢ä¸ºå‚æ•°åˆ—è¡¨ï¼ˆä¸åŸLFighterä¸€è‡´ï¼‰
        for i in range(m):
            local_models[i] = list(local_models[i].parameters())
        global_model = list(global_model.parameters())
        
        # è®¡ç®—æ¢¯åº¦å·®å¼‚ï¼ˆä¸åŸLFighterä¸€è‡´ï¼‰
        dw = [None for i in range(m)]
        db = [None for i in range(m)]
        for i in range(m):
            dw[i] = global_model[-2].cpu().data.numpy() - local_models[i][-2].cpu().data.numpy()
            db[i] = global_model[-1].cpu().data.numpy() - local_models[i][-1].cpu().data.numpy()
        dw = np.asarray(dw)
        db = np.asarray(db)

        # å¤„ç†äºŒåˆ†ç±»æƒ…å†µï¼ˆä¸åŸLFighterä¸€è‡´ï¼‰
        if len(db[0]) <= 2:
            data = []
            for i in range(m):
                data.append(dw[i].reshape(-1))
            
            # ä½¿ç”¨autoencoderè¿›è¡Œç‰¹å¾å­¦ä¹ 
            latent_features, reconstruction_errors, final_loss = self.train_autoencoder(data, device)
            
            # Use KMeans clustering (in latent space)
            from sklearn.cluster import KMeans
            
            # KMeansèšç±» (n_clusters=2)
            kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
            labels = kmeans.fit_predict(latent_features)
            
            print(f'[LFighter-AE] KMeansèšç±»: 2ä¸ªèšç±»')
            print(f'[LFighter-AE] èšç±»æ ‡ç­¾åˆ†å¸ƒ: {np.unique(labels)}')
            print(f'[LFighter-AE] å„æ ‡ç­¾æ•°é‡: {[np.sum(labels == label) for label in [0, 1]]}')
            
            # æ„å»ºèšç±»ç”¨äºè´¨é‡è¯„ä¼°
            clusters = {0: [], 1: []}
            for i, l in enumerate(labels):
                clusters[l].append(latent_features[i])
            
            # èšç±»è´¨é‡è¯„ä¼°
            good_cl = 0
            cs0, cs1 = self.clusters_dissimilarity(clusters)
            if cs0 < cs1:
                good_cl = 1

            # ç»“åˆèšç±»ç»“æœå’Œé‡æ„è¯¯å·®è®¡ç®—æœ€ç»ˆå¾—åˆ†
            cluster_scores = np.ones([m])
            for i, l in enumerate(labels):
                if l != good_cl:
                    cluster_scores[i] = 0
            
            # é‡æ„è¯¯å·®å½’ä¸€åŒ–ï¼ˆè¯¯å·®è¶Šå¤§ï¼Œå¾—åˆ†è¶Šä½ï¼‰
            normalized_recon_errors = (reconstruction_errors - reconstruction_errors.min()) / (reconstruction_errors.max() - reconstruction_errors.min() + 1e-8)
            recon_scores = 1 - normalized_recon_errors
            
            # ç»„åˆå¾—åˆ†ï¼šèšç±»å¾—åˆ†å’Œé‡æ„å¾—åˆ†åŠ æƒå¹³å‡
            final_scores = (1 - self.reconstruction_weight) * cluster_scores + self.reconstruction_weight * recon_scores
            
            print(f'[LFighter-AE] Cluster quality: cs0={cs0:.4f}, cs1={cs1:.4f}, good_cluster={good_cl}')
            print(f'[LFighter-AE] Reconstruction errors: mean={reconstruction_errors.mean():.4f}, std={reconstruction_errors.std():.4f}')
            
                        # å¯è§†åŒ–ç»“æœ
            if self.should_visualize_this_round():
                # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
                metrics = {
                    'total_clients': m,
                    'good_clients': int(np.sum(final_scores > 0.5)),
                    'accuracy': int(np.sum(final_scores > 0.5)) / m,
                    'cs0': cs0,
                    'cs1': cs1,
                    'good_cluster': good_cl,
                    'mean_recon_error': reconstruction_errors.mean(),
                    'std_recon_error': reconstruction_errors.std(),
                    'final_loss': final_loss,
                    'reduction_method': 'Binary classification',
                    'attack_type': 'Binary classification mode',
                    'fixed_recon_weight': self.reconstruction_weight,
                    # KMeansé…ç½®ä¿¡æ¯
                    'clustering_method': 'KMeans',
                    'n_clusters': 2,
                    'cluster_labels': str([0, 1]),
                    'cluster_counts': str([np.sum(labels == label) for label in [0, 1]])
                }
                
                # ä»…ç”ŸæˆPDFæŠ¥å‘Šï¼Œä¸ç”Ÿæˆå•ç‹¬çš„å›¾ç‰‡å’Œæ–‡æœ¬æ–‡ä»¶
                self.create_pdf_report(self.round_counter, latent_features, labels, ptypes, final_scores, cs0, cs1, good_cl, metrics)
            
            global_weights = average_weights(local_weights, final_scores)
            return global_weights

        # å¤„ç†å¤šåˆ†ç±»æƒ…å†µ
        # æ£€æµ‹å¼‚å¸¸ç±»åˆ«ï¼ˆæ”¹è¿›ç‰ˆï¼šæ”¯æŒå¤šæ ‡ç­¾æ”»å‡»ï¼‰
        norms = np.linalg.norm(dw, axis=-1)
        self.memory = np.sum(norms, axis=0)
        self.memory += np.sum(abs(db), axis=0)
        
        # é€šç”¨æ”»å‡»æ£€æµ‹ç­–ç•¥ï¼šè‡ªé€‚åº”é€‰æ‹©å±€éƒ¨vså…¨å±€ç‰¹å¾
        sorted_classes = self.memory.argsort()
        memory_normalized = self.memory / (np.max(self.memory) + 1e-8)
        
        # åˆ†ææ”»å‡»åˆ†å¸ƒæ¨¡å¼
        high_threshold = 0.6  # é«˜å½±å“é˜ˆå€¼
        medium_threshold = 0.3  # ä¸­ç­‰å½±å“é˜ˆå€¼
        
        high_impact_classes = sorted_classes[memory_normalized[sorted_classes] > high_threshold]
        medium_impact_classes = sorted_classes[memory_normalized[sorted_classes] > medium_threshold]
        
        # è®¡ç®—æ”»å‡»åˆ†å¸ƒçš„å‡åŒ€æ€§
        memory_std = np.std(memory_normalized)
        memory_cv = memory_std / (np.mean(memory_normalized) + 1e-8)  # å˜å¼‚ç³»æ•°
        
        print(f'[LFighter-AE] Memory scores: {dict(zip(range(len(self.memory)), self.memory))}')
        print(f'[LFighter-AE] Memory CV (å˜å¼‚ç³»æ•°): {memory_cv:.4f}')
        print(f'[LFighter-AE] High impact classes (>{high_threshold}): {high_impact_classes}')
        print(f'[LFighter-AE] Medium impact classes (>{medium_threshold}): {medium_impact_classes}')
        
        # æ ¹æ®æ”»å‡»åˆ†å¸ƒé€‰æ‹©ç‰¹å¾æå–ç­–ç•¥
        if memory_cv < 0.5 and len(medium_impact_classes) >= len(self.memory) * 0.6:
            # å…¨å±€æ”»å‡»æ¨¡å¼ï¼ˆå¦‚1ç§»ä½æ”»å‡»ï¼‰ï¼šæ‰€æœ‰ç±»åˆ«éƒ½å—åˆ°è¾ƒå‡åŒ€çš„å½±å“
            attack_scope = 'global'
            selected_classes = list(range(len(self.memory)))  # ä½¿ç”¨æ‰€æœ‰ç±»åˆ«
            print(f'[LFighter-AE] æ£€æµ‹åˆ°å…¨å±€æ”»å‡»æ¨¡å¼ï¼Œä½¿ç”¨æ‰€æœ‰{len(selected_classes)}ä¸ªç±»åˆ«')
        elif len(high_impact_classes) >= 4:
            # å¤§è§„æ¨¡å¤šæ ‡ç­¾æ”»å‡»
            attack_scope = 'large_multi'
            selected_classes = sorted_classes[-min(8, len(high_impact_classes) + 2):]
            print(f'[LFighter-AE] æ£€æµ‹åˆ°å¤§è§„æ¨¡å¤šæ ‡ç­¾æ”»å‡»ï¼Œä½¿ç”¨{len(selected_classes)}ä¸ªé«˜å½±å“ç±»åˆ«')
        elif len(high_impact_classes) >= 2:
            # å°è§„æ¨¡å¤šæ ‡ç­¾æ”»å‡»
            attack_scope = 'small_multi'
            selected_classes = sorted_classes[-min(6, len(high_impact_classes) + 1):]
            print(f'[LFighter-AE] æ£€æµ‹åˆ°å°è§„æ¨¡å¤šæ ‡ç­¾æ”»å‡»ï¼Œä½¿ç”¨{len(selected_classes)}ä¸ªç±»åˆ«')
        else:
            # ä¼ ç»Ÿå•ä¸€æ”»å‡»æˆ–æ— æ˜æ˜¾æ”»å‡»
            attack_scope = 'traditional'
            selected_classes = sorted_classes[-2:]
            print(f'[LFighter-AE] æ£€æµ‹åˆ°ä¼ ç»Ÿæ”»å‡»æ¨¡å¼ï¼Œä½¿ç”¨{len(selected_classes)}ä¸ªç±»åˆ«')
        
        # åˆ†ææ”»å‡»æ¨¡å¼ï¼ˆä½¿ç”¨æ–°çš„å‚æ•°ï¼‰
        attack_analysis = self.analyze_attack_pattern(selected_classes, self.memory, attack_scope, memory_cv)
        
        # æå–ç‰¹å¾ - ä¿®æ”¹ä¸ºå§‹ç»ˆä½¿ç”¨æ‰€æœ‰è¾“å‡ºå±‚æ¢¯åº¦
        data = []
        for i in range(m):
            # ä½¿ç”¨å®Œæ•´çš„æ¢¯åº¦å‘é‡ï¼Œä¸å†åŒºåˆ†æ”»å‡»æ¨¡å¼
            global_features = np.concatenate([dw[i].reshape(-1), db[i].reshape(-1)])
            data.append(global_features)
            
        print(f'[LFighter-AE] ä½¿ç”¨å…¨éƒ¨è¾“å‡ºå±‚æ¢¯åº¦ä½œä¸ºç‰¹å¾ (ç»´åº¦: {len(data[0])})')

        # ç»Ÿä¸€é™ç»´å¤„ç†ï¼ˆä¸åŸLFighterä¸€è‡´ï¼‰
        def unified_dimension_reduction(features, target_dim=200, method='auto', standardize=True):
            features_array = np.array(features)
            n_samples, original_dim = features_array.shape
            
            if standardize:
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                features_array = scaler.fit_transform(features_array)
                std_info = "standardized"
            else:
                std_info = "raw"
            
            max_pca_components = min(n_samples, original_dim)
            effective_target_dim = min(target_dim, max_pca_components)
            
            if original_dim <= effective_target_dim:
                return features_array, f"keep_all_{original_dim}_{std_info}"
            elif original_dim <= effective_target_dim * 2:
                return features_array[:, :effective_target_dim], f"truncate_{effective_target_dim}_{std_info}"
            else:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=effective_target_dim, random_state=42)
                reduced_features = pca.fit_transform(features_array)
                explained_ratio = np.sum(pca.explained_variance_ratio_)
                return reduced_features, f"pca_{effective_target_dim}_var{explained_ratio:.3f}_{std_info}"
        
        # åº”ç”¨é™ç»´
        data_reduced, reduction_method = unified_dimension_reduction(data, target_dim=200)
        print(f'[LFighter-AE] Applied dimension reduction: {reduction_method}')
        
        # ä½¿ç”¨autoencoderè¿›è¡Œæ·±åº¦ç‰¹å¾å­¦ä¹ 
        latent_features, reconstruction_errors, final_loss = self.train_autoencoder(data_reduced, device)
        
        # Use KMeans clustering (in latent space)
        from sklearn.cluster import KMeans
        
        # KMeansèšç±» (n_clusters=2)
        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
        labels = kmeans.fit_predict(latent_features)
        
        print(f'[LFighter-AE] KMeansèšç±»: 2ä¸ªèšç±»')
        print(f'[LFighter-AE] èšç±»æ ‡ç­¾åˆ†å¸ƒ: {np.unique(labels)}')
        print(f'[LFighter-AE] å„æ ‡ç­¾æ•°é‡: {[np.sum(labels == label) for label in [0, 1]]}')
        
        # æ„å»ºèšç±»ç”¨äºè´¨é‡è¯„ä¼°
        clusters = {0: [], 1: []}
        for i, l in enumerate(labels):
            clusters[l].append(latent_features[i])
        
        # èšç±»è´¨é‡è¯„ä¼°
        good_cl = 0
        cs0, cs1 = self.clusters_dissimilarity(clusters)
        if cs0 < cs1:
            good_cl = 1

        # ç»“åˆèšç±»ç»“æœå’Œé‡æ„è¯¯å·®
        cluster_scores = np.ones([m])
        for i, l in enumerate(labels):
            if l != good_cl:
                cluster_scores[i] = 0
        
        # é‡æ„è¯¯å·®å¾—åˆ†ï¼ˆè¯¯å·®è¶Šå¤§ï¼Œè¶Šå¯èƒ½æ˜¯æ”»å‡»è€…ï¼‰
        normalized_recon_errors = (reconstruction_errors - reconstruction_errors.min()) / (reconstruction_errors.max() - reconstruction_errors.min() + 1e-8)
        recon_scores = 1 - normalized_recon_errors
        
        # ä½¿ç”¨å›ºå®šæƒé‡ï¼ˆä¸è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼‰
        print(f'[LFighter-AE] ä½¿ç”¨å›ºå®šé‡æ„æƒé‡: {self.reconstruction_weight:.2f}')
        
        # ç»„åˆæœ€ç»ˆå¾—åˆ†
        final_scores = (1 - self.reconstruction_weight) * cluster_scores + self.reconstruction_weight * recon_scores
        
        # è®¾ç½®é˜ˆå€¼ï¼šåªæœ‰ç»„åˆå¾—åˆ†è¶…è¿‡0.5çš„å®¢æˆ·ç«¯è¢«è®¤ä¸ºæ˜¯å¥½çš„
        binary_scores = (final_scores > 0.5).astype(float)
        
        print(f'[LFighter-AE] Cluster quality: cs0={cs0:.4f}, cs1={cs1:.4f}, good_cluster={good_cl}')
        print(f'[LFighter-AE] Reconstruction errors: mean={reconstruction_errors.mean():.4f}, std={reconstruction_errors.std():.4f}')
        print(f'[LFighter-AE] Fixed reconstruction weight: {self.reconstruction_weight:.2f}')
        print(f'[LFighter-AE] Final scores: {final_scores}')
        print(f'[LFighter-AE] Selected good clients: {np.sum(binary_scores)}/{m}')
        
        # å¯è§†åŒ–ç»“æœ
        # å¯è§†åŒ–ç»“æœ
        if self.should_visualize_this_round():
            metrics = {
                'total_clients': m,
                'good_clients': int(np.sum(binary_scores)),
                'accuracy': int(np.sum(binary_scores)) / m,
                'cs0': cs0,
                'cs1': cs1,
                'good_cluster': good_cl,
                'mean_recon_error': reconstruction_errors.mean(),
                'std_recon_error': reconstruction_errors.std(),
                'final_loss': final_loss,
                'reduction_method': reduction_method,
                'feature_strategy': 'ä½¿ç”¨å…¨éƒ¨è¾“å‡ºå±‚æ¢¯åº¦',
                'feature_dim': len(data[0]),
                'selected_classes': str(selected_classes),  # ä»…ç”¨äºæ”»å‡»æ¨¡å¼åˆ†æ
                'num_selected_classes': len(selected_classes),
                'attack_scope': attack_scope,
                'memory_max': self.memory.max(),
                'memory_mean': self.memory.mean(),
                'attack_type': attack_analysis['attack_type'],
                'attack_confidence': attack_analysis['confidence'],
                'attack_description': attack_analysis['pattern_description'],
                'fixed_recon_weight': self.reconstruction_weight,
                # KMeansé…ç½®ä¿¡æ¯
                'clustering_method': 'KMeans',
                'n_clusters': 2,
                'cluster_labels': str([0, 1]),
                'cluster_counts': str([np.sum(labels == label) for label in [0, 1]])
            }
            
            # ä»…ç”ŸæˆPDFæŠ¥å‘Šï¼Œä¸ç”Ÿæˆå•ç‹¬çš„å›¾ç‰‡å’Œæ–‡æœ¬æ–‡ä»¶
            self.create_pdf_report(self.round_counter, latent_features, labels, ptypes, binary_scores, cs0, cs1, good_cl, metrics)
        
        global_weights = average_weights(local_weights, binary_scores)
        return global_weights

    def create_summary_report(self, round_num, metrics):
        """åˆ›å»ºæ€»ç»“æŠ¥å‘Š"""
        if not self.should_visualize_this_round():
            return
        
    def analyze_attack_pattern(self, selected_classes, memory_scores, attack_scope, memory_cv):
        """
        é€šç”¨æ”»å‡»æ¨¡å¼åˆ†æï¼Œæ”¯æŒå…¨å±€å’Œå±€éƒ¨æ”»å‡»
        
        æ³¨æ„ï¼šè™½ç„¶æ­¤æ–¹æ³•ä»ç„¶åˆ†ææ”»å‡»æ¨¡å¼ï¼Œä½†ç°åœ¨ç‰¹å¾æå–æ€»æ˜¯ä½¿ç”¨å…¨éƒ¨è¾“å‡ºå±‚æ¢¯åº¦ï¼Œ
        ä¸å†æ ¹æ®æ”»å‡»æ¨¡å¼é€‰æ‹©ä¸åŒçš„ç‰¹å¾å­é›†ã€‚æ­¤åˆ†æä»…ç”¨äºè°ƒæ•´é‡æ„æƒé‡å’Œæä¾›æŠ¥å‘Šä¿¡æ¯ã€‚
        
        Args:
            selected_classes: é€‰å®šçš„ç±»åˆ«ç´¢å¼•ï¼ˆä»…ç”¨äºåˆ†æï¼‰
            memory_scores: æ¯ä¸ªç±»åˆ«çš„å†…å­˜å¾—åˆ†
            attack_scope: æ”»å‡»èŒƒå›´ç±»å‹
            memory_cv: å†…å­˜å¾—åˆ†çš„å˜å¼‚ç³»æ•°
            
        Returns:
            åŒ…å«æ”»å‡»ç±»å‹ã€ç½®ä¿¡åº¦å’Œæè¿°çš„å­—å…¸
        """
        analysis = {
            'attack_type': 'unknown',
            'confidence': 0.0,
            'pattern_description': '',
            'scope': attack_scope
        }
        
        # æ ¹æ®æ”»å‡»èŒƒå›´è¿›è¡Œä¸åŒçš„åˆ†æ
        if attack_scope == 'global':
            # å…¨å±€æ”»å‡»åˆ†æï¼ˆå¦‚1ç§»ä½æ”»å‡»ï¼‰
            if memory_cv < 0.3:
                analysis['attack_type'] = 'global_uniform'
                analysis['confidence'] = 0.9
                analysis['pattern_description'] = f'å…¨å±€å‡åŒ€æ”»å‡» (CV={memory_cv:.3f})ï¼Œå¯èƒ½æ˜¯ç§»ä½æ”»å‡»æˆ–å…¨å±€æ ‡ç­¾ç¿»è½¬'
            else:
                analysis['attack_type'] = 'global_mixed'
                analysis['confidence'] = 0.7
                analysis['pattern_description'] = f'å…¨å±€æ··åˆæ”»å‡» (CV={memory_cv:.3f})ï¼Œå½±å“æ‰€æœ‰ç±»åˆ«ä½†ç¨‹åº¦ä¸å‡'
        else:
            # å±€éƒ¨æ”»å‡»åˆ†æ
            max_score = memory_scores.max()
            class_scores = memory_scores[selected_classes]
            relative_scores = class_scores / max_score if max_score > 0 else class_scores
            
            if attack_scope == 'large_multi':
                analysis['attack_type'] = 'large_multi_target'
                analysis['confidence'] = 0.85
                analysis['pattern_description'] = f'å¤§è§„æ¨¡å¤šæ ‡ç­¾æ”»å‡»ï¼Œæ¶‰åŠ{len(selected_classes)}ä¸ªç±»åˆ«'
            elif attack_scope == 'small_multi':
                high_score_classes = selected_classes[relative_scores > 0.7]
                if len(high_score_classes) >= 3:
                    analysis['attack_type'] = 'small_multi_target'
                    analysis['confidence'] = 0.8
                    analysis['pattern_description'] = f'å°è§„æ¨¡å¤šæ ‡ç­¾æ”»å‡»ï¼Œ{len(high_score_classes)}ä¸ªé«˜å½±å“ç±»åˆ«'
                else:
                    analysis['attack_type'] = 'complex_sparse'
                    analysis['confidence'] = 0.6
                    analysis['pattern_description'] = f'å¤æ‚ç¨€ç–æ”»å‡»æ¨¡å¼ï¼Œæ¶‰åŠ{len(selected_classes)}ä¸ªç±»åˆ«'
            elif attack_scope == 'traditional':
                analysis['attack_type'] = 'simple_targeted'
                analysis['confidence'] = 0.9
                analysis['pattern_description'] = 'ä¼ ç»Ÿå•æº-å•ç›®æ ‡æ”»å‡»'
            else:
                analysis['attack_type'] = 'unknown_local'
                analysis['confidence'] = 0.3
                analysis['pattern_description'] = 'æœªçŸ¥å±€éƒ¨æ”»å‡»æ¨¡å¼'
        
        print(f'[LFighter-AE] æ”»å‡»æ¨¡å¼åˆ†æ: {analysis["attack_type"]} (confidence: {analysis["confidence"]:.2f})')
        print(f'[LFighter-AE] æ¨¡å¼æè¿°: {analysis["pattern_description"]}')
        print(f'[LFighter-AE] æ”»å‡»èŒƒå›´: {attack_scope}')
        
        return analysis

